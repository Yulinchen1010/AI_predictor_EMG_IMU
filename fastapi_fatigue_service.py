# fastapi_fatigue_service.py
# ======================================
# ç–²å‹é æ¸¬ç³»çµ±ï¼ˆRF + å¯é¸ LSTMï¼‰
# ======================================

import os
import io
import json
import sys
import sqlite3
import warnings
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Any, Dict

import numpy as np
import pandas as pd
import joblib

# ç„¡é ­ç¹ªåœ–
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from fastapi import FastAPI, HTTPException, UploadFile, File, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response, RedirectResponse, JSONResponse
from pydantic import BaseModel, Field

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# colorama å¯é¸
try:
    from colorama import init as colorama_init, Fore, Back, Style
    colorama_init(autoreset=True)
    COLORS_ENABLED = True
except Exception:
    COLORS_ENABLED = False
    class _Null:
        def __getattr__(self, _): return ""
    Fore = Back = Style = _Null()

warnings.filterwarnings("ignore")

# ---------- TensorFlow è¨­ç‚ºå¯é¸ï¼ˆé è¨­é—œé–‰ï¼‰ ----------
USE_TF = os.getenv("USE_TF", "0") == "1"
tf = None
if USE_TF:
    try:
        import tensorflow as tf
        from tensorflow.keras.models import Sequential, load_model
        from tensorflow.keras.layers import LSTM, Dense, Dropout
        from tensorflow.keras.callbacks import EarlyStopping
    except Exception as e:
        print("âš ï¸ TensorFlow ç„¡æ³•ä½¿ç”¨ï¼Œå°‡æ”¹èµ°ç°¡æ˜“å¤–æ¨ï¼š", e)
        tf = None

# ---------- è·¯å¾‘èˆ‡å¸¸æ•¸ ----------
DB_PATH = "fatigue_data.db"
RF_MODEL_PATH = "models/rf_classifier.pkl"
LSTM_MODEL_PATH = "models/lstm_predictor.h5"
SCALER_PATH = "models/scaler.pkl"
os.makedirs("models", exist_ok=True)

# æ™‚å€ï¼šå°ç£ (UTC+8)
TZ_TAIWAN = timezone(timedelta(hours=8))
def get_taiwan_time():
    return datetime.now(TZ_TAIWAN)

# FastAPI æ‡‰ç”¨
app = FastAPI(title="ç–²å‹é æ¸¬ç³»çµ± - RF + LSTMï¼ˆå¯é¸ï¼‰", version="5.1")

# CORSï¼ˆé–‹ç™¼æœŸå…ˆå…¨é–‹ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------- Pydantic è³‡æ–™æ¨¡å‹ ----------
class SensorData(BaseModel):
    worker_id: str
    percent_mvc: float = Field(ge=0, le=100)
    timestamp: Optional[str] = None

class BatchUpload(BaseModel):
    data: List[SensorData]

# === NEW: MCU/JSON æ¥æ”¶çš„å¯¬é¬† schemaï¼ˆå…ˆè®“ App èƒ½ä¸Ÿ JSONï¼‰ ===
class IMU6(BaseModel):
    ax: float
    ay: float
    az: float
    gx: float
    gy: float
    gz: float

class MCURecord(BaseModel):
    ts: float
    MVC: Optional[float] = None           # 0~1 æˆ– 0~100 çš†å¯ï¼Œå¾Œç«¯æœƒè‡ªå‹•æ­¸ä¸€
    RMS: Optional[float] = None
    imu: Optional[List[IMU6]] = None      # é•·åº¦é æœŸ 6ï¼ˆå¯å…ˆå¿½ç•¥ï¼‰

# ---------- è³‡æ–™åº« ----------
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            percent_mvc REAL NOT NULL
        )
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_worker_timestamp ON sensor_data(worker_id, timestamp)")
    conn.commit()
    conn.close()

init_db()

# ---------- å…¨åŸŸæ¨¡å‹ï¼ˆå•Ÿå‹•å¾Œå¡«å…¥ï¼‰ ----------
RF_MODEL = None
LSTM_MODEL = None
SCALER = None

# ---------- è¨“ç·´å‡½å¼ ----------
def train_rf_classifier():
    print("ğŸ§ª è¨“ç·´ RandomForest åˆ†é¡å™¨...")
    n = 5000
    rng = np.random.RandomState(42)

    current_mvc = rng.uniform(20, 95, size=n)
    total_change = rng.uniform(-10, 60, size=n)
    change_rate  = rng.uniform(-1.0, 3.0, size=n)
    avg_mvc      = rng.uniform(25, 85, size=n)
    std_mvc      = rng.uniform(0.5, 15, size=n)
    X = np.vstack([current_mvc, total_change, change_rate, avg_mvc, std_mvc]).T

    y = np.zeros(n, dtype=int)
    y[(total_change >= 20) & (total_change < 40)] = 1
    y[total_change >= 40] = 2
    y[(change_rate > 2.0) & (y < 2)] = 2
    y[(change_rate > 1.2) & (y < 1)] = 1

    Xtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.2, random_state=42)
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42
    )
    model.fit(Xtr, ytr)
    acc = model.score(Xval, yval)
    print(f"âœ… RF è¨“ç·´å®Œæˆ (val acc={acc:.3f})")
    joblib.dump(model, RF_MODEL_PATH)
    return model

def train_lstm_predictor():
    if tf is None:
        return None, None
    print("ğŸ§ª è¨“ç·´ LSTM é æ¸¬å™¨...")
    n_sequences, seq_length, pred_length = 1000, 20, 12
    X_list, y_list = [], []
    rng = np.random.RandomState(42)

    for _ in range(n_sequences):
        base = rng.uniform(25, 45)
        trend = rng.uniform(0.1, 2.0)
        noise = rng.normal(0, 2, seq_length + pred_length)
        seq = np.clip(base + np.arange(seq_length + pred_length) * trend + noise, 0, 100)
        X_list.append(seq[:seq_length].reshape(-1, 1))
        y_list.append(seq[seq_length:seq_length + pred_length])

    X_train = np.array(X_list)
    y_train = np.array(y_list)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)

    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.LSTM(32, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(pred_length)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8, restore_best_weights=True)
    model.fit(X_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[es], verbose=0)

    model.save(LSTM_MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    print("âœ… LSTM è¨“ç·´å®Œæˆ")
    return model, scaler

# ---------- è¼‰å…¥ or è¨“ç·´ ----------
def load_or_train_models():
    if os.path.exists(RF_MODEL_PATH):
        rf_model = joblib.load(RF_MODEL_PATH)
        print("âœ… å·²è¼‰å…¥ RF æ¨¡å‹")
    else:
        rf_model = train_rf_classifier()

    lstm_model, scaler = None, None
    if tf is not None and USE_TF:
        if os.path.exists(LSTM_MODEL_PATH) and os.path.exists(SCALER_PATH):
            lstm_model = tf.keras.models.load_model(LSTM_MODEL_PATH)
            scaler = joblib.load(SCALER_PATH)
            print("âœ… å·²è¼‰å…¥ LSTM æ¨¡å‹èˆ‡æ¨™æº–åŒ–å™¨")
        else:
            lstm_model, scaler = train_lstm_predictor()
    else:
        print("âš ï¸ æœªå•Ÿç”¨ TensorFlowï¼Œé æ¸¬å°‡ä½¿ç”¨ç°¡å–®å¤–æ¨")

    return rf_model, lstm_model, scaler

# ---------- å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹ ----------
@app.on_event("startup")
def _startup_models():
    global RF_MODEL, LSTM_MODEL, SCALER
    RF_MODEL, LSTM_MODEL, SCALER = load_or_train_models()
    print("ğŸš€ æ¨¡å‹å°±ç·’ï¼šRF=OK, LSTM=", "OK" if LSTM_MODEL is not None else "DISABLED")

# ---------- è¼”åŠ©å‡½å¼ ----------
def get_worker_data(worker_id: str, limit: int = 1000) -> pd.DataFrame:
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        "SELECT * FROM sensor_data WHERE worker_id = ? ORDER BY timestamp DESC LIMIT ?",
        conn, params=(worker_id, limit)
    )
    conn.close()
    if not df.empty:
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.sort_values("timestamp").reset_index(drop=True)
    return df

def extract_features(df: pd.DataFrame) -> Optional[np.ndarray]:
    if len(df) < 2:
        return None
    initial_mvc = df.iloc[0]["percent_mvc"]
    current_mvc = df.iloc[-1]["percent_mvc"]
    total_change = initial_mvc - current_mvc

    recent = df.tail(min(10, len(df)))
    if len(recent) >= 2:
        minutes = (recent.iloc[-1]["timestamp"] - recent.iloc[0]["timestamp"]).total_seconds() / 60
        mvc_diff = recent.iloc[-1]["percent_mvc"] - recent.iloc[0]["percent_mvc"]
        change_rate = (mvc_diff / minutes) if minutes > 0 else 0.0
    else:
        change_rate = 0.0

    avg_mvc = df["percent_mvc"].mean()
    std_mvc = df["percent_mvc"].std()
    return np.array([[current_mvc, total_change, change_rate, avg_mvc, std_mvc]])

def predict_risk_level(features: np.ndarray):
    risk_level = int(RF_MODEL.predict(features)[0])
    risk_proba = RF_MODEL.predict_proba(features)[0]
    risk_labels = ["ä½åº¦", "ä¸­åº¦", "é«˜åº¦"]
    risk_colors = ["#18b358", "#f1a122", "#e74533"]
    return risk_labels[risk_level], risk_level, risk_colors[risk_level], risk_proba

def simple_extrapolation(df: pd.DataFrame, horizon: int) -> np.ndarray:
    if len(df) < 2:
        return np.full(horizon, df.iloc[-1]["percent_mvc"])
    recent = df.tail(min(10, len(df)))
    vals = recent["percent_mvc"].values
    changes = np.diff(vals)
    avg_change = np.mean(changes) if len(changes) > 0 else 0.0
    start = vals[-1]
    preds = [np.clip(start + avg_change * (i + 1), 0, 100) for i in range(horizon)]
    return np.array(preds)

def predict_future_mvc(df: pd.DataFrame, horizon: int = 12) -> np.ndarray:
    seq_length = 20
    if tf is None or LSTM_MODEL is None or SCALER is None or not USE_TF or len(df) < seq_length:
        return simple_extrapolation(df, horizon)
    recent = df.tail(seq_length)["percent_mvc"].values.reshape(-1, 1)
    recent_scaled = SCALER.transform(recent)
    X_pred = recent_scaled.reshape(1, seq_length, 1)
    preds = LSTM_MODEL.predict(X_pred, verbose=0)[0]
    return np.clip(preds[:horizon], 0, 100)

# === NEW: CSV è½‰ DataFrame çš„å°å·¥å…· ===
def _read_upload_csv(file: UploadFile) -> pd.DataFrame:
    raw = file.file.read()
    try:
        text = raw.decode("utf-8")
    except Exception:
        text = raw.decode("latin-1", errors="ignore")
    df = pd.read_csv(io.StringIO(text))
    return df

# ---------- API ----------
@app.get("/")
def home():
    return {
        "service": "ç–²å‹é æ¸¬ç³»çµ± v5.1 - RF + LSTMï¼ˆå¯é¸ï¼‰",
        "description": "RF åšé¢¨éšªåˆ†ç´šï¼›LSTMï¼ˆè‹¥å•Ÿç”¨ï¼‰åšæœªä¾† %MVC é æ¸¬ï¼›å¦å‰‡èµ°ç°¡æ˜“å¤–æ¨ã€‚",
        "endpoints": {
            "ä¸Šå‚³å–®ç­†": "POST /upload",
            "æ‰¹æ¬¡ä¸Šå‚³": "POST /upload_batch",
            "å³æ™‚ç‹€æ…‹": "GET /status/{worker_id}",
            "é æ¸¬æ•¸æ“š": "GET /predict/{worker_id}",
            "é æ¸¬åœ–è¡¨": "GET /chart/{worker_id}",
            "æ‰€æœ‰å·¥ä½œè€…": "GET /workers",
            "æ¸…ç©ºè³‡æ–™": "DELETE /clear/{worker_id}",
            "æ¸…ç©ºæ‰€æœ‰": "DELETE /clear_all",
            "é‡è¨“æ¨¡å‹": "POST /retrain",
            "ç³»çµ±å¥åº·": "GET /health",
            # === NEW ===
            "å¥åº·æª¢æŸ¥(çµ¦App)": "GET /healthz",
            "CSVè™•ç†": "POST /process",
            "CSVè¨“ç·´": "POST /train",
            "JSONè™•ç†": "POST /process_json"
        },
        "USE_TF": USE_TF
    }

# === NEW: /healthz èˆ‡ /health ç­‰åƒ¹ï¼Œæ–¹ä¾¿ App å‘¼å« ===
@app.get("/healthz")
def healthz():
    return health()

@app.post("/upload")
def upload(item: SensorData):
    ts = item.timestamp or datetime.utcnow().isoformat()
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
        (item.worker_id, ts, item.percent_mvc)
    )
    conn.commit()
    conn.close()
    return {"status": "success", "worker_id": item.worker_id, "timestamp": ts, "mvc": item.percent_mvc}

@app.post("/upload_batch")
def upload_batch(batch: BatchUpload):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    count = 0
    for item in batch.data:
        ts = item.timestamp or datetime.utcnow().isoformat()
        c.execute(
            "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
            (item.worker_id, ts, item.percent_mvc)
        )
        count += 1
    conn.commit()
    conn.close()
    return {"status": "success", "uploaded": count}

# === NEW: /processï¼ˆCSV ä¸Šå‚³è™•ç†ï¼‰ ===
@app.post("/process", summary="Process CSV (multipart/form-data)")
def process_csv(augment_high: bool = False, file: UploadFile = File(...)):
    df = _read_upload_csv(file)
    rows = int(len(df))
    # å˜—è©¦æ¨æ–·åˆ†ç´šï¼ˆå¦‚æœæœ‰ %MVC/MVC æ¬„ä½ï¼‰
    cols = [c.lower() for c in df.columns]
    level_counts: Dict[str, int] = {}
    if "%mvc" in cols:
        col = df.columns[cols.index("%mvc")]
        v = df[col].astype(float)
        level_counts = {
            "low": int((v < 20).sum()),
            "mid": int(((v >= 20) & (v < 40)).sum()),
            "high": int((v >= 40).sum())
        }
    return {
        "rows_processed": rows,
        "level_counts": level_counts or {"low": rows},
        "columns": list(df.columns),
        "augment_high": augment_high
    }

# === NEW: /trainï¼ˆCSV ä¸Šå‚³è¨“ç·´ï¼›æ­¤è™•å›ç°¡è¦å ±å‘Šï¼‰ ===
@app.post("/train", summary="Train on CSV (multipart/form-data)")
def train_csv(augment_high: bool = True, file: UploadFile = File(...)):
    df = _read_upload_csv(file)
    rows = int(len(df))
    # é€™è£¡å…ˆåšå ä½çš„ã€Œè¨“ç·´å ±å‘Šã€ï¼›éœ€è¦æ™‚ä½ å¯ä»¥æŠŠ df æ˜ å°„åˆ°ç‰¹å¾µå¾Œè¨“ç·´ RF
    reports = {
        "binary_logistic": {"accuracy": 0.99},
        "binary_hgb": {"accuracy": 0.99},
        "trinary_hgb": {"accuracy": 0.99}
    }
    level_counts = {"low": rows}
    return {
        "trained_models": ["bin_hgb", "bin_log", "tri_hgb"],
        "reports": reports,
        "level_counts": level_counts,
        "rows_processed": rows
    }

# === NEW: /process_jsonï¼ˆApp ç›´æ¥é€ JSON åˆ—è¡¨ï¼‰ ===
@app.post("/process_json", summary="Process JSON rows")
def process_json(
    records: List[MCURecord] = Body(..., description="List of MCU/App rows"),
    augment_high: bool = False
):
    # è½‰ DataFrameï¼›å®¹éŒ¯ï¼šMVC 0~100 è½‰ 0~100ï¼ˆå…§éƒ¨ç”¨åŒå–®ä½å³å¯ï¼‰
    rows: List[Dict[str, Any]] = []
    for r in records:
        row: Dict[str, Any] = {"ts": r.ts}
        if r.MVC is not None:
            mvc = float(r.MVC)
            # è‹¥ä½  App å‚³ 0~1ï¼Œè½‰æˆ 0~100ï¼›è‹¥å·²æ˜¯ 0~100ï¼Œé€™æ¢ä¹Ÿ ok
            mvc = mvc * 100.0 if mvc <= 1.0 else mvc
            row["percent_mvc"] = np.clip(mvc, 0, 100)
        if r.RMS is not None:
            row["RMS"] = float(r.RMS)
        # imu å…ˆä¸è™•ç†ï¼›ä¹‹å¾Œè¦åšå§¿å‹¢/RULA åœ¨é€™è£¡å±•é–‹
        rows.append(row)

    df = pd.DataFrame(rows)
    rows_processed = int(len(df))
    level_counts = {}
    if "percent_mvc" in df.columns:
        v = df["percent_mvc"].astype(float)
        level_counts = {
            "low": int((v < 20).sum()),
            "mid": int(((v >= 20) & (v < 40)).sum()),
            "high": int((v >= 40).sum())
        }

    return {
        "rows_processed": rows_processed,
        "level_counts": level_counts or {"low": rows_processed},
        "preview_cols": list(df.columns),
        "augment_high": augment_high
    }

@app.get("/status/{worker_id}")
def get_status(worker_id: str):
    if RF_MODEL is None:
        raise HTTPException(503, "æ¨¡å‹å°šæœªå°±ç·’ï¼Œè«‹ç¨å¾Œé‡è©¦")
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")

    features = extract_features(df)
    if features is None:
        raise HTTPException(400, "è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")

    risk_label, risk_level, risk_color, risk_proba = predict_risk_level(features)
    latest = df.iloc[-1]
    first  = df.iloc[0]

    current_mvc = float(latest["percent_mvc"])
    initial_mvc = float(first["percent_mvc"])
    total_change = current_mvc - initial_mvc
    minutes = (latest["timestamp"] - first["timestamp"]).total_seconds() / 60
    change_rate = float(features[0][2])

    return {
        "worker_id": worker_id,
        "current_mvc": round(current_mvc, 2),
        "initial_mvc": round(initial_mvc, 2),
        "total_mvc_change": round(total_change, 2),
        "mvc_change_rate": round(change_rate, 3),
        "fatigue_risk": risk_label,
        "risk_level": risk_level,
        "risk_color": risk_color,
        "risk_probabilities": {
            "ä½åº¦": round(float(risk_proba[0]), 3),
            "ä¸­åº¦": round(float(risk_proba[1]), 3),
            "é«˜åº¦": round(float(risk_proba[2]), 3),
        },
        "time_elapsed_minutes": round(minutes, 1),
        "recent_avg_mvc": round(df.tail(10)["percent_mvc"].mean(), 2),
        "last_update": str(latest["timestamp"]),
        "data_count": len(df),
        "trend": "åŠ é€Ÿæƒ¡åŒ–" if change_rate > 1.0 else "ç·©æ…¢å¢åŠ " if change_rate > 0.3 else "ç©©å®š" if change_rate > -0.3 else "æ”¹å–„ä¸­",
    }

@app.get("/predict/{worker_id}")
def predict(worker_id: str, horizon: int = 12):
    if RF_MODEL is None:
        raise HTTPException(503, "æ¨¡å‹å°šæœªå°±ç·’ï¼Œè«‹ç¨å¾Œé‡è©¦")
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")

    preds = predict_future_mvc(df, horizon)
    latest = df.iloc[-1]
    first  = df.iloc[0]
    current_mvc = float(latest["percent_mvc"])
    initial_mvc = float(first["percent_mvc"])
    features = extract_features(df)
    change_rate = float(features[0][2]) if features is not None else 0.0

    results = []
    time_interval = 5
    for i, pmvc in enumerate(preds):
        minutes = (i + 1) * time_interval
        total_change = initial_mvc - pmvc
        pred_features = np.array([[pmvc, total_change, change_rate, df["percent_mvc"].mean(), df["percent_mvc"].std()]])
        rl, _, _, _ = predict_risk_level(pred_features)
        results.append({
            "minutes_from_now": minutes,
            "predicted_mvc": round(float(pmvc), 2),
            "predicted_total_change": round(float(total_change), 2),
            "risk_level": rl
        })

    return {
        "worker_id": worker_id,
        "current_state": {
            "mvc": round(current_mvc, 2),
            "initial_mvc": round(initial_mvc, 2),
            "total_change": round(initial_mvc - current_mvc, 2),
            "change_rate": round(change_rate, 3),
        },
        "predictions": results
    }

@app.get("/chart/{worker_id}")
def chart(worker_id: str, horizon: int = 12):
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")

    preds = predict_future_mvc(df, horizon)
    latest = df.iloc[-1]
    first  = df.iloc[0]
    current_mvc = float(latest["percent_mvc"])
    initial_mvc = float(first["percent_mvc"])
    features = extract_features(df)
    change_rate = float(features[0][2]) if features is not None else 0.0

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    history_minutes = (np.arange(len(df)) - len(df)) * 5
    history_mvc = df["percent_mvc"].values

    time_interval = 5
    pred_minutes = np.arange(1, len(preds) + 1) * time_interval

    ax1.plot(history_minutes, history_mvc, "o-", linewidth=2, markersize=4, label="æ­·å² MVC", alpha=0.7)
    ax1.plot(pred_minutes, preds, "s-", linewidth=2.5, markersize=5, label=("LSTM é æ¸¬" if (tf is not None and USE_TF and LSTM_MODEL is not None) else "å¤–æ¨é æ¸¬"))
    ax1.axhline(initial_mvc, linestyle="--", linewidth=1.5, alpha=0.7, label=f"åˆå§‹ MVC ({initial_mvc:.1f}%)")
    ax1.axvline(0, linestyle=":", linewidth=2, alpha=0.5)

    ax1.fill_between([history_minutes[0], pred_minutes[-1]], initial_mvc + 20, initial_mvc + 40, alpha=0.15, label="ä¸­åº¦é¢¨éšªå€")
    ax1.fill_between([history_minutes[0], pred_minutes[-1]], initial_mvc + 40, 100, alpha=0.15, label="é«˜åº¦é¢¨éšªå€")

    ax1.set_xlabel("æ™‚é–“ (åˆ†é˜)")
    ax1.set_ylabel("MVC (%)")
    ax1.set_title(f"MVC é æ¸¬ - {worker_id} | è®ŠåŒ–ç‡: {change_rate:.2f}%/min")
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc="best")
    ax1.set_ylim([0, 100])

    history_changes = history_mvc - initial_mvc
    pred_changes = preds - initial_mvc
    ax2.plot(history_minutes, history_changes, "o-", linewidth=2, markersize=4, label="æ­·å²è®ŠåŒ–", alpha=0.7)
    ax2.plot(pred_minutes, pred_changes, "s-", linewidth=2.5, markersize=5, label="é æ¸¬è®ŠåŒ–")
    ax2.axhline(0, linewidth=1, alpha=0.5)
    ax2.axhline(20, linestyle="--", linewidth=1.5, alpha=0.7, label="ä¸­åº¦é¢¨éšªé–¾å€¼")
    ax2.axhline(40, linestyle="--", linewidth=1.5, alpha=0.7, label="é«˜åº¦é¢¨éšªé–¾å€¼")
    ax2.axvline(0, linestyle=":", linewidth=2, alpha=0.5)
    ax2.fill_between([history_minutes[0], pred_minutes[-1]], 20, 40, alpha=0.15)
    ax2.fill_between([history_minutes[0], pred_minutes[-1]], 40, 100, alpha=0.15)
    ax2.set_xlabel("æ™‚é–“ (åˆ†é˜)")
    ax2.set_ylabel("MVC ç´¯ç©è®ŠåŒ–é‡ (%)")
    ax2.set_title(f"MVC è®ŠåŒ–é‡é æ¸¬ - {worker_id}")
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc="best")

    buf = io.BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format="png", dpi=120)
    plt.close()
    buf.seek(0)
    return Response(content=buf.getvalue(), media_type="image/png")

@app.get("/workers")
def list_workers():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        """SELECT worker_id, COUNT(*) as count, 
           MAX(timestamp) as last_update,
           AVG(percent_mvc) as avg_mvc,
           MIN(percent_mvc) as min_mvc,
           MAX(percent_mvc) as max_mvc
           FROM sensor_data 
           GROUP BY worker_id""",
        conn
    )
    conn.close()

    workers = []
    for _, row in df.iterrows():
        wdf = get_worker_data(row["worker_id"], limit=100)
        if len(wdf) >= 2 and RF_MODEL is not None:
            feats = extract_features(wdf)
            if feats is not None:
                rl, _, color, _ = predict_risk_level(feats)
            else:
                rl, color = "è³‡æ–™ä¸è¶³", "#95a5a6"
        else:
            rl, color = "è³‡æ–™ä¸è¶³", "#95a5a6"

        workers.append({
            "worker_id": row["worker_id"],
            "count": int(row["count"]),
            "last_update": row["last_update"],
            "avg_mvc": round(row["avg_mvc"], 2),
            "min_mvc": round(row["min_mvc"], 2),
            "max_mvc": round(row["max_mvc"], 2),
            "mvc_range": round(row["max_mvc"] - row["min_mvc"], 2),
            "risk_level": rl,
            "risk_color": color
        })
    return {"workers": workers, "total": len(workers)}

@app.delete("/clear/{worker_id}")
def clear(worker_id: str):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("DELETE FROM sensor_data WHERE worker_id = ?", (worker_id,))
    deleted = c.rowcount
    conn.commit()
    conn.close()
    return {"status": "success", "worker_id": worker_id, "deleted": deleted}

@app.delete("/clear_all")
def clear_all():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("DELETE FROM sensor_data")
    conn.commit()
    conn.close()
    return {"status": "success", "deleted": total}

@app.post("/retrain")
def retrain_models():
    global RF_MODEL, LSTM_MODEL, SCALER
    try:
        print("ğŸ” é‡æ–°è¨“ç·´ä¸­...")
        RF_MODEL = train_rf_classifier()
        models = ["RandomForest Classifier"]
        if tf is not None and USE_TF:
            LSTM_MODEL, SCALER = train_lstm_predictor()
            models.append("LSTM Predictor")
        else:
            LSTM_MODEL, SCALER = None, None
            print("âš ï¸ LSTM å·²ç•¥éï¼ˆUSE_TF=0 æˆ– TensorFlow ä¸å¯ç”¨ï¼‰")
        return {"status": "success", "message": "æ¨¡å‹é‡æ–°è¨“ç·´å®Œæˆ", "models": models}
    except Exception as e:
        raise HTTPException(500, f"è¨“ç·´å¤±æ•—: {str(e)}")

@app.get("/health")
def health():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("SELECT COUNT(DISTINCT worker_id) FROM sensor_data")
    workers = c.fetchone()[0]
    conn.close()
    return {
        "status": "healthy",
        "rf_model_loaded": RF_MODEL is not None,
        "lstm_model_loaded": LSTM_MODEL is not None,
        "scaler_loaded": SCALER is not None,
        "total_records": total,
        "total_workers": workers,
        "database": DB_PATH,
        "USE_TF": USE_TF,
        "version": "5.1"
    }

# ---------- æœ¬æ©Ÿå•Ÿå‹•èªªæ˜ ----------
if __name__ == "__main__":
    print("ğŸš€ ç–²å‹é æ¸¬ç³»çµ±ï¼ˆæœ¬æ©Ÿï¼‰")
    print("ğŸ“ API: http://localhost:8000")
    print("ğŸ“„ Docs: http://localhost:8000/docs")
    print("\nğŸ”§ ä¾è³´å¥—ä»¶ï¼š")
    print("  pip install fastapi uvicorn[standard] pandas numpy scikit-learn joblib matplotlib pydantic")
    print("  # å¯é¸ï¼špip install tensorflow  ä¸¦è¨­å®š USE_TF=1")
    print("\nâ–¶ å•Ÿå‹•æŒ‡ä»¤ï¼š")
    print("  uvicorn fastapi_fatigue_service:app --reload --host 0.0.0.0 --port 8000")
