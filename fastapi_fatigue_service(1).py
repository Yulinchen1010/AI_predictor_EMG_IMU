from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from typing import Optional, List
import sqlite3
import pandas as pd
import numpy as np
import os
import joblib
from datetime import datetime, timedelta, timezone
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings('ignore')

# çµ‚ç«¯æ©Ÿé¡¯ç¤ºè¨­å®š
import sys
from colorama import init, Fore, Back, Style
try:
    init(autoreset=True)
    COLORS_ENABLED = True
except:
    COLORS_ENABLED = False
    print("âš ï¸ colorama æœªå®‰è£ï¼Œä½¿ç”¨ç´”æ–‡å­—é¡¯ç¤º (å¯é¸: pip install colorama)")

# ==================== åˆå§‹è¨­å®š ====================
DB_PATH = "fatigue_data.db"
RF_MODEL_PATH = "models/rf_classifier.pkl"
LSTM_MODEL_PATH = "models/lstm_predictor.h5"
SCALER_PATH = "models/scaler.pkl"
os.makedirs("models", exist_ok=True)

# è¨­å®šæ™‚å€ï¼šå°ç£æ™‚é–“ (UTC+8)
TZ_TAIWAN = timezone(timedelta(hours=8))

def get_taiwan_time():
    """å–å¾—å°ç£ç•¶å‰æ™‚é–“"""
    return datetime.now(TZ_TAIWAN)

app = FastAPI(title="ç–²å‹é æ¸¬ç³»çµ± - RF + LSTM")

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== è³‡æ–™æ¨¡å‹ ====================
class SensorData(BaseModel):
    worker_id: str
    percent_mvc: float = Field(ge=0, le=100)
    timestamp: Optional[str] = None

class BatchUpload(BaseModel):
    data: List[SensorData]

# ==================== è³‡æ–™åº«åˆå§‹åŒ– ====================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            percent_mvc REAL NOT NULL
        )
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_worker_timestamp ON sensor_data(worker_id, timestamp)")
    conn.commit()
    conn.close()

init_db()

# ==================== æ¨¡å‹è¨“ç·´å‡½æ•¸ ====================
def train_rf_classifier():
    """è¨“ç·´ RandomForest é¢¨éšªåˆ†é¡å™¨"""
    print("è¨“ç·´ RandomForest åˆ†é¡å™¨...")
    
    # ç”Ÿæˆè¨“ç·´è³‡æ–™
    n = 5000
    rng = np.random.RandomState(42)
    
    # ç‰¹å¾µï¼šç•¶å‰MVCã€ç´¯ç©è®ŠåŒ–é‡ã€è®ŠåŒ–é€Ÿç‡ã€å¹³å‡MVCã€æ¨™æº–å·®
    current_mvc = rng.uniform(20, 95, size=n)
    total_change = rng.uniform(-10, 60, size=n)
    change_rate = rng.uniform(-1.0, 3.0, size=n)
    avg_mvc = rng.uniform(25, 85, size=n)
    std_mvc = rng.uniform(0.5, 15, size=n)
    
    X = np.vstack([current_mvc, total_change, change_rate, avg_mvc, std_mvc]).T
    
    # ç›®æ¨™ï¼šé¢¨éšªç­‰ç´š (0: ä½åº¦, 1: ä¸­åº¦, 2: é«˜åº¦)
    y = np.zeros(n, dtype=int)
    y[(total_change >= 20) & (total_change < 40)] = 1
    y[total_change >= 40] = 2
    # è€ƒæ…®è®ŠåŒ–é€Ÿç‡çš„å½±éŸ¿
    y[(change_rate > 2.0) & (y < 2)] = 2
    y[(change_rate > 1.2) & (y < 1)] = 1
    
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42
    )
    model.fit(X_train, y_train)
    
    score = model.score(X_val, y_val)
    print(f"RandomForest åˆ†é¡å™¨è¨“ç·´å®Œæˆ (æº–ç¢ºç‡ = {score:.3f})")
    
    joblib.dump(model, RF_MODEL_PATH)
    return model

def train_lstm_predictor():
    """è¨“ç·´ LSTM æ™‚é–“åºåˆ—é æ¸¬å™¨"""
    print("è¨“ç·´ LSTM é æ¸¬å™¨...")
    
    # ç”Ÿæˆè¨“ç·´è³‡æ–™
    n_sequences = 1000
    seq_length = 20
    pred_length = 12  # é æ¸¬æœªä¾† 12 å€‹æ™‚é–“é»
    
    X_train_list = []
    y_train_list = []
    
    rng = np.random.RandomState(42)
    
    for _ in range(n_sequences):
        # ç”Ÿæˆä¸€å€‹æ™‚é–“åºåˆ—
        base = rng.uniform(25, 45)
        trend = rng.uniform(0.1, 2.0)
        noise = rng.normal(0, 2, seq_length + pred_length)
        
        sequence = base + np.arange(seq_length + pred_length) * trend + noise
        sequence = np.clip(sequence, 0, 100)
        
        X_train_list.append(sequence[:seq_length].reshape(-1, 1))
        y_train_list.append(sequence[seq_length:seq_length + pred_length])
    
    X_train = np.array(X_train_list)
    y_train = np.array(y_train_list)
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
    
    # å»ºç«‹ LSTM æ¨¡å‹
    model = Sequential([
        LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),
        Dropout(0.2),
        LSTM(32, activation='relu'),
        Dropout(0.2),
        Dense(pred_length)
    ])
    
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)
    
    history = model.fit(
        X_train_scaled, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stop],
        verbose=0
    )
    
    print(f"LSTM é æ¸¬å™¨è¨“ç·´å®Œæˆ (MAE = {history.history['mae'][-1]:.3f})")
    
    model.save(LSTM_MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    
    return model, scaler

# ==================== è¼‰å…¥/è¨“ç·´æ¨¡å‹ ====================
def load_or_train_models():
    """è¼‰å…¥æˆ–è¨“ç·´æ¨¡å‹"""
    # RandomForest åˆ†é¡å™¨
    if os.path.exists(RF_MODEL_PATH):
        print("âœ… è¼‰å…¥ RandomForest åˆ†é¡å™¨")
        rf_model = joblib.load(RF_MODEL_PATH)
    else:
        rf_model = train_rf_classifier()
    
    # LSTM é æ¸¬å™¨
    if os.path.exists(LSTM_MODEL_PATH) and os.path.exists(SCALER_PATH):
        print("âœ… è¼‰å…¥ LSTM é æ¸¬å™¨")
        lstm_model = load_model(LSTM_MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
    else:
        lstm_model, scaler = train_lstm_predictor()
    
    return rf_model, lstm_model, scaler

# åˆå§‹åŒ–æ¨¡å‹
RF_MODEL, LSTM_MODEL, SCALER = load_or_train_models()

# ==================== è¼”åŠ©å‡½æ•¸ ====================
def get_worker_data(worker_id: str, limit: int = 1000) -> pd.DataFrame:
    """å–å¾—å·¥ä½œè€…è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        f"SELECT * FROM sensor_data WHERE worker_id = ? ORDER BY timestamp DESC LIMIT {limit}",
        conn, params=(worker_id,)
    )
    conn.close()
    if not df.empty:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp').reset_index(drop=True)
    return df

def extract_features(df: pd.DataFrame) -> np.ndarray:
    """å¾è³‡æ–™ä¸­æå–ç‰¹å¾µç”¨æ–¼ RF åˆ†é¡"""
    if len(df) < 2:
        return None
    
    initial_mvc = df.iloc[-1]['percent_mvc']
    current_mvc = df.iloc[0]['percent_mvc']
    total_change = initial_mvc - current_mvc
    
    # è¨ˆç®—è®ŠåŒ–é€Ÿç‡ï¼ˆæœ€è¿‘10ç­†ï¼‰
    recent_df = df.tail(min(10, len(df)))
    if len(recent_df) >= 2:
        time_diff = (recent_df.iloc[-1]['timestamp'] - recent_df.iloc[0]['timestamp']).total_seconds() / 60
        if time_diff > 0:
            mvc_diff = recent_df.iloc[-1]['percent_mvc'] - recent_df.iloc[0]['percent_mvc']
            change_rate = mvc_diff / time_diff
        else:
            change_rate = 0.0
    else:
        change_rate = 0.0
    
    avg_mvc = df['percent_mvc'].mean()
    std_mvc = df['percent_mvc'].std()
    
    features = np.array([[current_mvc, total_change, change_rate, avg_mvc, std_mvc]])
    return features

def predict_risk_level(features: np.ndarray) -> tuple:
    """ä½¿ç”¨ RF é æ¸¬é¢¨éšªç­‰ç´š"""
    risk_level = int(RF_MODEL.predict(features)[0])
    risk_proba = RF_MODEL.predict_proba(features)[0]
    
    risk_labels = ["ä½åº¦", "ä¸­åº¦", "é«˜åº¦"]
    risk_colors = ["#18b358", "#f1a122", "#e74533"]
    
    return risk_labels[risk_level], risk_level, risk_colors[risk_level], risk_proba

def predict_future_mvc(df: pd.DataFrame, horizon: int = 12) -> np.ndarray:
    """ä½¿ç”¨ LSTM é æ¸¬æœªä¾† MVC å€¼"""
    seq_length = 20
    
    if len(df) < seq_length:
        # è³‡æ–™ä¸è¶³ï¼Œä½¿ç”¨ç°¡å–®ç·šæ€§å¤–æ¨
        return simple_extrapolation(df, horizon)
    
    # å–æœ€è¿‘çš„åºåˆ—
    recent_mvc = df.tail(seq_length)['percent_mvc'].values.reshape(-1, 1)
    
    # æ¨™æº–åŒ–
    recent_scaled = SCALER.transform(recent_mvc)
    
    # é æ¸¬
    X_pred = recent_scaled.reshape(1, seq_length, 1)
    predictions = LSTM_MODEL.predict(X_pred, verbose=0)[0]
    
    # é™åˆ¶ç¯„åœ
    predictions = np.clip(predictions[:horizon], 0, 100)
    
    return predictions

def simple_extrapolation(df: pd.DataFrame, horizon: int) -> np.ndarray:
    """ç°¡å–®ç·šæ€§å¤–æ¨ï¼ˆè³‡æ–™ä¸è¶³æ™‚ä½¿ç”¨ï¼‰"""
    if len(df) < 2:
        return np.full(horizon, df.iloc[-1]['percent_mvc'])
    
    recent = df.tail(min(10, len(df)))
    mvc_values = recent['percent_mvc'].values
    
    # è¨ˆç®—å¹³å‡è®ŠåŒ–ç‡
    changes = np.diff(mvc_values)
    avg_change = np.mean(changes) if len(changes) > 0 else 0
    
    current_mvc = mvc_values[0]
    predictions = []
    
    for i in range(1, horizon + 1):
        pred = current_mvc + avg_change * i
        predictions.append(np.clip(pred, 0, 100))
    
    return np.array(predictions)

def get_recommendation(risk_level: int, change_rate: float) -> str:
    """æ ¹æ“šé¢¨éšªç­‰ç´šå’Œè®ŠåŒ–è¶¨å‹¢æä¾›å»ºè­°"""
    if risk_level == 2 or change_rate > 2.0:
        return "âš ï¸ é«˜é¢¨éšªï¼å»ºè­°ç«‹å³ä¼‘æ¯ 15-20 åˆ†é˜"
    elif risk_level == 1 or change_rate > 1.2:
        return "âš¡ ä¸­åº¦é¢¨éšªï¼Œå»ºè­°èª¿æ•´å·¥ä½œå§¿å‹¢æˆ–çŸ­æš«ä¼‘æ¯"
    elif change_rate < 0:
        return "âœ… ç‹€æ…‹æ”¹å–„ä¸­ï¼Œç¹¼çºŒä¿æŒ"
    else:
        return "âœ… ç‹€æ…‹è‰¯å¥½ï¼Œç¹¼çºŒä¿æŒ"

# ==================== API ç«¯é» ====================

@app.get("/")
def home():
    return {
        "service": "ç–²å‹é æ¸¬ç³»çµ± v5.0 - RF + LSTM",
        "description": "ä½¿ç”¨ RandomForest åˆ†é¡é¢¨éšªç­‰ç´šï¼ŒLSTM é æ¸¬æ™‚é–“åºåˆ—",
        "features": [
            "âœ… RandomForest å¤šé¡åˆ¥é¢¨éšªåˆ†é¡",
            "âœ… LSTM æ™‚é–“åºåˆ—é æ¸¬",
            "âœ… å¯¦æ™‚é¢¨éšªè©•ä¼°èˆ‡é æ¸¬",
            "âœ… è¦–è¦ºåŒ–é æ¸¬æ›²ç·š"
        ],
        "endpoints": {
            "ä¸Šå‚³å–®ç­†": "POST /upload",
            "æ‰¹æ¬¡ä¸Šå‚³": "POST /upload_batch",
            "å³æ™‚ç‹€æ…‹": "GET /status/{worker_id}",
            "é æ¸¬æ•¸æ“š": "GET /predict/{worker_id}",
            "é æ¸¬åœ–è¡¨": "GET /chart/{worker_id}",
            "æ‰€æœ‰å·¥ä½œè€…": "GET /workers",
            "æ¸…ç©ºè³‡æ–™": "DELETE /clear/{worker_id}",
            "æ¸…ç©ºæ‰€æœ‰": "DELETE /clear_all",
            "é‡è¨“æ¨¡å‹": "POST /retrain",
            "ç³»çµ±å¥åº·": "GET /health"
        }
    }

@app.post('/upload')
def upload(item: SensorData):
    """ä¸Šå‚³å–®ç­†æ„Ÿæ¸¬å™¨è³‡æ–™"""
    ts = item.timestamp or datetime.utcnow().isoformat()
    
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
        (item.worker_id, ts, item.percent_mvc)
    )
    conn.commit()
    conn.close()
    
    print(f"âœ… ä¸Šå‚³æˆåŠŸ: {item.worker_id} | MVC={item.percent_mvc}% | æ™‚é–“={ts}")
    
    return {
        "status": "success",
        "worker_id": item.worker_id,
        "timestamp": ts,
        "mvc": item.percent_mvc
    }

@app.post('/upload_batch')
def upload_batch(batch: BatchUpload):
    """æ‰¹æ¬¡ä¸Šå‚³å¤šç­†è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    for item in batch.data:
        ts = item.timestamp or datetime.utcnow().isoformat()
        c.execute(
            "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
            (item.worker_id, ts, item.percent_mvc)
        )
    
    conn.commit()
    conn.close()
    
    print(f"âœ… æ‰¹æ¬¡ä¸Šå‚³æˆåŠŸ: {len(batch.data)} ç­†")
    
    return {
        "status": "success",
        "uploaded": len(batch.data)
    }

@app.get('/status/{worker_id}')
def get_status(worker_id: str):
    """å–å¾—å·¥ä½œè€…å³æ™‚ç‹€æ…‹ï¼ˆä½¿ç”¨ RF åˆ†é¡ï¼‰"""
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")
    
    # æå–ç‰¹å¾µ
    features = extract_features(df)
    if features is None:
        raise HTTPException(400, "è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")
    
    # RF åˆ†é¡é æ¸¬
    risk_label, risk_level, risk_color, risk_proba = predict_risk_level(features)
    
    # åŸºæœ¬çµ±è¨ˆ
    latest = df.iloc[-1]
    initial = df.iloc[0]
    current_mvc = float(latest['percent_mvc'])
    initial_mvc = float(initial['percent_mvc'])
    total_change = current_mvc - initial_mvc
    
    time_diff = (latest['timestamp'] - initial['timestamp']).total_seconds() / 60
    
    # è®ŠåŒ–é€Ÿç‡
    change_rate = float(features[0][2])
    
    return {
        "worker_id": worker_id,
        "current_mvc": round(current_mvc, 2),
        "initial_mvc": round(initial_mvc, 2),
        "total_mvc_change": round(total_change, 2),
        "mvc_change_rate": round(change_rate, 3),
        "fatigue_risk": risk_label,
        "risk_level": risk_level,
        "risk_color": risk_color,
        "risk_probabilities": {
            "ä½åº¦": round(float(risk_proba[0]), 3),
            "ä¸­åº¦": round(float(risk_proba[1]), 3),
            "é«˜åº¦": round(float(risk_proba[2]), 3)
        },
        "time_elapsed_minutes": round(time_diff, 1),
        "recent_avg_mvc": round(df.tail(10)['percent_mvc'].mean(), 2),
        "last_update": str(latest['timestamp']),
        "data_count": len(df),
        "trend": "åŠ é€Ÿæƒ¡åŒ–" if change_rate > 1.0 else "ç·©æ…¢å¢åŠ " if change_rate > 0.3 else "ç©©å®š" if change_rate > -0.3 else "æ”¹å–„ä¸­",
        "recommendation": get_recommendation(risk_level, change_rate)
    }

@app.get('/predict/{worker_id}')
def predict(worker_id: str, horizon: int = 12):
    """å–å¾—æœªä¾†MVCè®ŠåŒ–é æ¸¬ï¼ˆä½¿ç”¨ LSTMï¼‰"""
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")
    
    # LSTM é æ¸¬
    predictions = predict_future_mvc(df, horizon)
    
    # ç•¶å‰ç‹€æ…‹
    latest = df.iloc[-1]
    initial = df.iloc[0]
    current_mvc = float(latest['percent_mvc'])
    initial_mvc = float(initial['percent_mvc'])
    
    features = extract_features(df)
    change_rate = float(features[0][2])
    
    # ç”Ÿæˆé æ¸¬çµæœ
    results = []
    time_interval = 5  # å‡è¨­æ¯å€‹é æ¸¬é»é–“éš”5åˆ†é˜
    
    for i, pred_mvc in enumerate(predictions):
        minutes = (i + 1) * time_interval
        total_change = initial_mvc - pred_mvc 
        
        # é æ¸¬è©²æ™‚åˆ»çš„é¢¨éšªç­‰ç´š
        pred_features = np.array([[pred_mvc, total_change, change_rate, 
                                  df['percent_mvc'].mean(), df['percent_mvc'].std()]])
        risk_label, risk_level, _, _ = predict_risk_level(pred_features)
        
        results.append({
            "minutes_from_now": minutes,
            "predicted_mvc": round(float(pred_mvc), 2),
            "predicted_total_change": round(float(total_change), 2),
            "risk_level": risk_label
        })
    
    return {
        "worker_id": worker_id,
        "current_state": {
            "mvc": current_mvc,
            "initial_mvc": initial_mvc,
            "total_change": round(initial_mvc-current_mvc, 2),
            "change_rate": round(change_rate, 3),
            "trend": "ä¸Šå‡" if change_rate > 0.1 else "ä¸‹é™" if change_rate < -0.1 else "ç©©å®š"
        },
        "predictions": results
    }

@app.get('/chart/{worker_id}')
def chart(worker_id: str, horizon: int = 12):
    """å–å¾—MVCè®ŠåŒ–é æ¸¬æ›²ç·šåœ–"""
    df = get_worker_data(worker_id)
    if df.empty:
        raise HTTPException(404, f"æ‰¾ä¸åˆ° {worker_id} çš„è³‡æ–™")
    
    # LSTM é æ¸¬
    predictions = predict_future_mvc(df, horizon)
    
    # ç•¶å‰ç‹€æ…‹
    latest = df.iloc[-1]
    initial = df.iloc[0]
    current_mvc = float(latest['percent_mvc'])
    initial_mvc = float(initial['percent_mvc'])
    
    features = extract_features(df)
    change_rate = float(features[0][2])
    
    # ç¹ªåœ–
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # æ­·å²è³‡æ–™
    history_minutes = np.arange(-len(df), 0) * 5
    history_mvc = df['percent_mvc'].values
    
    # é æ¸¬è³‡æ–™
    time_interval = 5
    pred_minutes = np.arange(1, len(predictions) + 1) * time_interval
    
    # åœ–1: MVCçµ•å°å€¼
    ax1.plot(history_minutes, history_mvc, 'o-', color='#3498db', linewidth=2, 
             markersize=4, label='æ­·å² MVC', alpha=0.7)
    ax1.plot(pred_minutes, predictions, 's-', color='#e74c3c', linewidth=2.5, 
             markersize=5, label='LSTM é æ¸¬')
    ax1.axhline(initial_mvc, color='green', linestyle='--', linewidth=1.5, 
                alpha=0.7, label=f'åˆå§‹ MVC ({initial_mvc:.1f}%)')
    ax1.axvline(0, color='gray', linestyle=':', linewidth=2, alpha=0.5)
    
    # é¢¨éšªå€åŸŸ
    ax1.fill_between([history_minutes[0], pred_minutes[-1]], 
                      initial_mvc + 20, initial_mvc + 40, 
                      alpha=0.15, color='orange', label='ä¸­åº¦é¢¨éšªå€')
    ax1.fill_between([history_minutes[0], pred_minutes[-1]], 
                      initial_mvc + 40, 100, 
                      alpha=0.15, color='red', label='é«˜åº¦é¢¨éšªå€')
    
    ax1.set_xlabel('æ™‚é–“ (åˆ†é˜)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('MVC (%)', fontsize=12, fontweight='bold')
    ax1.set_title(f'MVC é æ¸¬ - {worker_id} | è®ŠåŒ–ç‡: {change_rate:.2f}%/min | LSTMé æ¸¬', 
                  fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best')
    ax1.set_ylim([0, 100])
    
    # åœ–2: MVCè®ŠåŒ–é‡
    history_changes = history_mvc - initial_mvc
    pred_changes = predictions - initial_mvc
    
    ax2.plot(history_minutes, history_changes, 'o-', color='#3498db', 
             linewidth=2, markersize=4, label='æ­·å²è®ŠåŒ–', alpha=0.7)
    ax2.plot(pred_minutes, pred_changes, 's-', color='#e74c3c', 
             linewidth=2.5, markersize=5, label='é æ¸¬è®ŠåŒ–')
    ax2.axhline(0, color='gray', linestyle='-', linewidth=1, alpha=0.5)
    ax2.axhline(20, color='orange', linestyle='--', linewidth=1.5, 
                alpha=0.7, label='ä¸­åº¦é¢¨éšªé–¾å€¼')
    ax2.axhline(40, color='red', linestyle='--', linewidth=1.5, 
                alpha=0.7, label='é«˜åº¦é¢¨éšªé–¾å€¼')
    ax2.axvline(0, color='gray', linestyle=':', linewidth=2, alpha=0.5)
    
    # é¢¨éšªå€åŸŸ
    ax2.fill_between([history_minutes[0], pred_minutes[-1]], 
                      20, 40, alpha=0.15, color='orange')
    ax2.fill_between([history_minutes[0], pred_minutes[-1]], 
                      40, 100, alpha=0.15, color='red')
    
    ax2.set_xlabel('æ™‚é–“ (åˆ†é˜)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('MVC ç´¯ç©è®ŠåŒ–é‡ (%)', fontsize=12, fontweight='bold')
    ax2.set_title(f'MVC è®ŠåŒ–é‡é æ¸¬ - {worker_id}', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best')
    
    buf = io.BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png', dpi=120)
    plt.close()
    buf.seek(0)
    
    return Response(content=buf.getvalue(), media_type='image/png')

@app.get('/workers')
def list_workers():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œè€…"""
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        """SELECT worker_id, COUNT(*) as count, 
           MAX(timestamp) as last_update,
           AVG(percent_mvc) as avg_mvc,
           MIN(percent_mvc) as min_mvc,
           MAX(percent_mvc) as max_mvc
           FROM sensor_data 
           GROUP BY worker_id""",
        conn
    )
    conn.close()
    
    workers = []
    for _, row in df.iterrows():
        worker_df = get_worker_data(row['worker_id'], limit=100)
        
        if len(worker_df) >= 2:
            features = extract_features(worker_df)
            risk_label, risk_level, risk_color, _ = predict_risk_level(features)
        else:
            risk_label = "è³‡æ–™ä¸è¶³"
            risk_color = "#95a5a6"
        
        workers.append({
            "worker_id": row['worker_id'],
            "count": int(row['count']),
            "last_update": row['last_update'],
            "avg_mvc": round(row['avg_mvc'], 2),
            "min_mvc": round(row['min_mvc'], 2),
            "max_mvc": round(row['max_mvc'], 2),
            "mvc_range": round(row['max_mvc'] - row['min_mvc'], 2),
            "risk_level": risk_label,
            "risk_color": risk_color
        })
    
    return {"workers": workers, "total": len(workers)}

@app.delete('/clear/{worker_id}')
def clear(worker_id: str):
    """æ¸…ç©ºå·¥ä½œè€…è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("DELETE FROM sensor_data WHERE worker_id = ?", (worker_id,))
    deleted = c.rowcount
    conn.commit()
    conn.close()
    
    print(f"ğŸ—‘ï¸ å·²åˆªé™¤ {worker_id} çš„ {deleted} ç­†è³‡æ–™")
    
    return {
        "status": "success",
        "worker_id": worker_id,
        "deleted": deleted
    }

@app.delete('/clear_all')
def clear_all():
    """æ¸…ç©ºæ‰€æœ‰è³‡æ–™"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("DELETE FROM sensor_data")
    conn.commit()
    conn.close()
    
    print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºè³‡æ–™åº«ï¼Œåˆªé™¤ {total} ç­†è³‡æ–™")
    
    return {
        "status": "success",
        "deleted": total
    }

@app.post('/retrain')
def retrain_models():
    """é‡æ–°è¨“ç·´æ¨¡å‹"""
    global RF_MODEL, LSTM_MODEL, SCALER
    
    try:
        print("ğŸ”„ é–‹å§‹é‡æ–°è¨“ç·´æ¨¡å‹...")
        
        # è¨“ç·´ RandomForest
        RF_MODEL = train_rf_classifier()
        
        # è¨“ç·´ LSTM
        LSTM_MODEL, SCALER = train_lstm_predictor()
        
        return {
            "status": "success",
            "message": "æ¨¡å‹é‡æ–°è¨“ç·´å®Œæˆ",
            "models": ["RandomForest Classifier", "LSTM Predictor"]
        }
    except Exception as e:
        raise HTTPException(500, f"è¨“ç·´å¤±æ•—: {str(e)}")

@app.get('/health')
def health():
    """ç³»çµ±å¥åº·æª¢æŸ¥"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    c.execute("SELECT COUNT(DISTINCT worker_id) FROM sensor_data")
    workers = c.fetchone()[0]
    conn.close()
    
    return {
        "status": "healthy",
        "rf_model_loaded": RF_MODEL is not None,
        "lstm_model_loaded": LSTM_MODEL is not None,
        "scaler_loaded": SCALER is not None,
        "total_records": total,
        "total_workers": workers,
        "database": DB_PATH,
        "version": "5.0 - RF + LSTM"
    }

if __name__ == '__main__':
    print("ğŸš€ ç–²å‹é æ¸¬ç³»çµ±å·²å•Ÿå‹•")
    print("ğŸ“ æœ¬æ©Ÿ: http://localhost:8000")
    print("ğŸ“ æ–‡ä»¶: http://localhost:8000/docs")
    print("\nâœ¨ v5.0 æ¶æ§‹:")
    print("  âœ… RandomForest: å¤šé¡åˆ¥é¢¨éšªåˆ†é¡ (ä½/ä¸­/é«˜åº¦)")
    print("  âœ… LSTM: æ™‚é–“åºåˆ—é æ¸¬æœªä¾† MVC è¶¨å‹¢")
    print("  âœ… å³æ™‚é¢¨éšªè©•ä¼°èˆ‡æ©Ÿç‡è¼¸å‡º")
    print("  âœ… å®Œæ•´çš„æ­·å²+é æ¸¬è¦–è¦ºåŒ–")
    print("\nâš™ï¸ ä¾è³´å¥—ä»¶:")
    print("  pip install fastapi uvicorn pandas numpy scikit-learn")
    print("  pip install tensorflow matplotlib joblib")
    print("\nğŸ¯ å•Ÿå‹•å‘½ä»¤:")
    print("  uvicorn main:app --reload --host 0.0.0.0 --port 8000")