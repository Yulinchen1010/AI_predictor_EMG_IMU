from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional
import sqlite3
import pandas as pd
import numpy as np
import os
import joblib
from datetime import datetime, timedelta, timezone
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings('ignore')

# ==================== åˆå§‹è¨­å®š ====================
DB_PATH = "fatigue_data.db"
RF_MODEL_PATH = "models/rf_classifier.pkl"
LSTM_MODEL_PATH = "models/lstm_predictor.keras"
SCALER_PATH = "models/scaler.pkl"
os.makedirs("models", exist_ok=True)

# å›ºå®šä½¿ç”¨è€… ID
SINGLE_USER_ID = "user001"

# é æ¸¬åƒæ•¸
TIME_INTERVAL_MINUTES = 1
HISTORY_SEQUENCE_LENGTH = 60
PREDICTION_HORIZON = 30
MAX_DATA_LIMIT = 1000

# å°ç£æ™‚å€
TZ_TAIWAN = timezone(timedelta(hours=8))

def get_taiwan_time():
    return datetime.now(TZ_TAIWAN)

app = FastAPI(title="ç–²å‹é æ¸¬ç³»çµ±")

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== è³‡æ–™æ¨¡å‹ ====================
class SensorData(BaseModel):
    percent_mvc: float = Field(ge=0, le=100)
    timestamp: Optional[str] = None

# ==================== è³‡æ–™åº«åˆå§‹åŒ– ====================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            percent_mvc REAL NOT NULL
        )
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_worker_timestamp ON sensor_data(worker_id, timestamp)")
    conn.commit()
    conn.close()

init_db()

# ==================== æ¨¡å‹è¨“ç·´ ====================
def train_rf_classifier():
    print("è¨“ç·´ RandomForest åˆ†é¡å™¨...")
    n = 5000
    rng = np.random.RandomState(42)
    
    current_mvc = rng.uniform(20, 95, size=n)
    total_change = rng.uniform(-10, 60, size=n)
    change_rate = rng.uniform(-1.0, 3.0, size=n)
    avg_mvc = rng.uniform(25, 85, size=n)
    std_mvc = rng.uniform(0.5, 15, size=n)
    
    X = np.vstack([current_mvc, total_change, change_rate, avg_mvc, std_mvc]).T
    y = np.zeros(n, dtype=int)
    y[(total_change >= 20)] = 1
    y[(total_change >= 40)] = 2
    y[(change_rate > 2.0) & (y < 2)] = 2
    y[(change_rate > 1.2) & (y < 1)] = 1
    
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=10, 
                                   min_samples_leaf=5, random_state=42)
    model.fit(X_train, y_train)
    
    score = model.score(X_val, y_val)
    print(f"RandomForest è¨“ç·´å®Œæˆ (æº–ç¢ºç‡ = {score:.3f})")
    joblib.dump(model, RF_MODEL_PATH)
    return model

def train_lstm_predictor():
    print("è¨“ç·´ LSTM é æ¸¬å™¨...")
    n_sequences = 1000
    seq_length = HISTORY_SEQUENCE_LENGTH
    pred_length = PREDICTION_HORIZON
    
    X_train_list = []
    y_train_list = []
    rng = np.random.RandomState(42)
    
    for _ in range(n_sequences):
        base = rng.uniform(60, 90)
        trend = rng.uniform(-2.0, -0.1)
        noise = rng.normal(0, 2, seq_length + pred_length)
        sequence = base + np.arange(seq_length + pred_length) * trend + noise
        sequence = np.clip(sequence, 0, 100)
        
        X_train_list.append(sequence[:seq_length].reshape(-1, 1))
        y_train_list.append(sequence[seq_length:seq_length + pred_length])
    
    X_train = np.array(X_train_list)
    y_train = np.array(y_train_list)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
    
    model = Sequential([
        LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),
        Dropout(0.2),
        LSTM(32, activation='relu'),
        Dropout(0.2),
        Dense(pred_length)
    ])
    
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)
    
    history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, 
                       validation_split=0.2, callbacks=[early_stop], verbose=0)
    
    final_mae = history.history.get('mean_absolute_error', [0])[-1]
    print(f"LSTM è¨“ç·´å®Œæˆ (MAE = {final_mae:.3f})")
    
    model.save(LSTM_MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    return model, scaler

def load_or_train_models():
    if os.path.exists(RF_MODEL_PATH):
        print("âœ… è¼‰å…¥ RandomForest")
        rf_model = joblib.load(RF_MODEL_PATH)
    else:
        rf_model = train_rf_classifier()
    
    if os.path.exists(LSTM_MODEL_PATH) and os.path.exists(SCALER_PATH):
        print("âœ… è¼‰å…¥ LSTM")
        lstm_model = load_model(LSTM_MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
    else:
        lstm_model, scaler = train_lstm_predictor()
    
    return rf_model, lstm_model, scaler

RF_MODEL, LSTM_MODEL, SCALER = load_or_train_models()

# ==================== è¼”åŠ©å‡½æ•¸ ====================
def get_worker_data(limit: int = None) -> pd.DataFrame:
    if limit is None:
        limit = MAX_DATA_LIMIT
    
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        f"SELECT * FROM sensor_data WHERE worker_id = ? ORDER BY timestamp DESC LIMIT {limit}",
        conn, params=(SINGLE_USER_ID,)
    )
    conn.close()
    if not df.empty:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp', ascending=True).reset_index(drop=True)
    return df

def extract_features(df: pd.DataFrame) -> np.ndarray:
    if len(df) < 2:
        return None
    
    first_mvc = df.iloc[0]['percent_mvc']
    last_mvc = df.iloc[-1]['percent_mvc']
    
    if first_mvc > last_mvc:
        initial_mvc = first_mvc
        current_mvc = last_mvc
    else:
        initial_mvc = df['percent_mvc'].max()
        current_mvc = last_mvc
    
    total_change = initial_mvc - current_mvc
    
    recent_df = df.tail(min(10, len(df)))
    if len(recent_df) >= 2:
        time_diff = (recent_df.iloc[-1]['timestamp'] - recent_df.iloc[0]['timestamp']).total_seconds() / 60
        if time_diff > 0:
            mvc_diff = recent_df.iloc[0]['percent_mvc'] - recent_df.iloc[-1]['percent_mvc']
            change_rate = mvc_diff / time_diff
        else:
            change_rate = 0.0
    else:
        change_rate = 0.0
    
    avg_mvc = df['percent_mvc'].mean()
    std_mvc = df['percent_mvc'].std()
    
    features = np.array([[current_mvc, total_change, change_rate, avg_mvc, std_mvc]])
    return features

def predict_risk_level(features: np.ndarray) -> tuple:
    risk_level = int(RF_MODEL.predict(features)[0])
    risk_proba = RF_MODEL.predict_proba(features)[0]
    risk_labels = ["ä½åº¦", "ä¸­åº¦", "é«˜åº¦"]
    risk_colors = ["#18b358", "#f1a122", "#e74533"]
    return risk_labels[risk_level], risk_level, risk_colors[risk_level], risk_proba

# ==================== API ç«¯é»ï¼ˆç°¡åŒ–ç‰ˆï¼‰====================

@app.get("/")
def home():
    return {
        "service": "ç–²å‹é æ¸¬ç³»çµ±",
        "version": "ç°¡åŒ–ç‰ˆ v1.0",
        "user_id": SINGLE_USER_ID,
        "endpoints": {
            "appä½¿ç”¨": "GET /app_data - App å°ˆç”¨ç«¯é»",
            "ä¸Šå‚³è³‡æ–™": "POST /upload",
            "å¥åº·æª¢æŸ¥": "GET /health"
        }
    }

# ==================== ğŸ¯ App å°ˆç”¨ç«¯é»ï¼ˆåªè¿”å›ä¸‰å€‹è³‡æ–™ï¼‰====================
@app.get("/app_data")
def get_app_data():
    """
    App å°ˆç”¨ç«¯é»
    åªè¿”å›ï¼šuser_id, last_update, risk_level
    """
    try:
        df = get_worker_data(limit=100)
        
        # å¦‚æœæ²’æœ‰è³‡æ–™
        if df.empty:
            return {
                "user_id": SINGLE_USER_ID,
                "last_update": None,
                "risk_level": "ç„¡è³‡æ–™"
            }
        
        # å¦‚æœè³‡æ–™ä¸è¶³
        features = extract_features(df)
        if features is None:
            return {
                "user_id": SINGLE_USER_ID,
                "last_update": str(df.iloc[-1]['timestamp']),
                "risk_level": "è³‡æ–™ä¸è¶³"
            }
        
        # æ­£å¸¸æƒ…æ³ï¼šé æ¸¬é¢¨éšªç­‰ç´š
        risk_label, risk_level, risk_color, _ = predict_risk_level(features)
        
        return {
            "user_id": SINGLE_USER_ID,
            "last_update": str(df.iloc[-1]['timestamp']),
            "risk_level": risk_label
        }
        
    except Exception as e:
        raise HTTPException(500, detail=f"æŸ¥è©¢å¤±æ•—: {str(e)}")

# ==================== ä¸Šå‚³è³‡æ–™ç«¯é» ====================
@app.post('/upload')
def upload(item: SensorData):
    """
    ä¸Šå‚³ MVC è³‡æ–™
    ä¸éœ€è¦æä¾› user_idï¼Œç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨å›ºå®šçš„ä½¿ç”¨è€…
    """
    ts = item.timestamp or get_taiwan_time().isoformat()
    
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT INTO sensor_data (worker_id, timestamp, percent_mvc) VALUES (?, ?, ?)",
        (SINGLE_USER_ID, ts, item.percent_mvc)
    )
    conn.commit()
    conn.close()
    
    print(f"âœ… ä¸Šå‚³æˆåŠŸ: MVC={item.percent_mvc}%")
    
    return {
        "status": "success",
        "user_id": SINGLE_USER_ID,
        "timestamp": ts,
        "mvc": item.percent_mvc
    }

# ==================== å¥åº·æª¢æŸ¥ ====================
@app.get("/healthz")
def healthz():
    # å›ç”¨ /health çš„è³‡è¨Šä¹Ÿå¯ä»¥ï¼š
    # return health()
    return {"status": "ok"}

@app.get('/health')
def health():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data WHERE worker_id = ?", (SINGLE_USER_ID,))
    total = c.fetchone()[0]
    conn.close()
    
    return {
        "status": "healthy",
        "user_id": SINGLE_USER_ID,
        "total_records": total,
        "models_loaded": RF_MODEL is not None and LSTM_MODEL is not None,
        "version": "ç°¡åŒ–ç‰ˆ v1.0"
    }

# ==================== æ¸…ç©ºè³‡æ–™ï¼ˆæ¸¬è©¦ç”¨ï¼‰====================
@app.delete('/clear')
def clear_data():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("DELETE FROM sensor_data WHERE worker_id = ?", (SINGLE_USER_ID,))
    deleted = c.rowcount
    conn.commit()
    conn.close()
    print(f"ğŸ—‘ï¸ å·²åˆªé™¤ {deleted} ç­†è³‡æ–™")
    return {"status": "success", "deleted": deleted}

if __name__ == '__main__':
    print("ğŸš€ ç–²å‹é æ¸¬ç³»çµ±å•Ÿå‹•ï¼ˆç°¡åŒ–ç‰ˆï¼‰")
    print(f"ğŸ‘¤ ä½¿ç”¨è€…: {SINGLE_USER_ID}")
    print("ğŸ“ æœ¬æ©Ÿ: http://localhost:8000")
    print("ğŸ“ App ç«¯é»: http://localhost:8000/app_data")
    print("\nğŸ¯ å•Ÿå‹•å‘½ä»¤:")

    print("  uvicorn main:app --reload --host 0.0.0.0 --port 8000")
