# main.py
from __future__ import annotations

import logging
import os
import sqlite3
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple, Union

import joblib
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# ==================== åŸºæœ¬è¨­å®š ====================
APP_TITLE = "ç–²å‹é æ¸¬ç³»çµ±"
DB_PATH = os.environ.get("DB_PATH", "fatigue_data.db")

# å»ºç½®ç‰ˆæœ¬ï¼šRender/Heroku å¸¸è¦‹ç’°å¢ƒè®Šæ•¸æˆ–é€€å› ISO æ™‚é–“
APP_BUILD = (
    os.environ.get("RENDER_GIT_COMMIT")
    or os.environ.get("SOURCE_VERSION")
    or os.environ.get("APP_BUILD")
    or datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
)

# æ¨¡å‹æª”æ¡ˆ
MODEL_DIR = os.environ.get("MODEL_DIR", "models")
RF_MODEL_PATH = os.path.join(MODEL_DIR, "rf_classifier.pkl")
# ğŸ”® LSTM èˆ‡ç¸®æ”¾å™¨ï¼ˆè‹¥å­˜åœ¨å°±è¼‰å…¥ï¼Œä¸å­˜åœ¨å°±ç”¨å¾Œæ´æ³•ï¼‰
LSTM_MODEL_PATH = os.path.join(MODEL_DIR, "lstm_predictor.keras")
SCALER_PATH = os.path.join(MODEL_DIR, "scaler.pkl")
os.makedirs(MODEL_DIR, exist_ok=True)

# é æ¸¬ç›¸é—œå¸¸æ•¸
TIME_INTERVAL_MINUTES = int(os.environ.get("TIME_INTERVAL_MINUTES", "1"))  # å–æ¨£ç²’åº¦ï¼ˆåˆ†ï¼‰
HISTORY_SEQUENCE_LENGTH = int(os.environ.get("HISTORY_SEQUENCE_LENGTH", "60"))  # LSTM æ­·å²é•·åº¦
PREDICTION_HORIZON = int(os.environ.get("PREDICTION_HORIZON", "30"))  # æœªä¾†å¹¾æ­¥ï¼ˆåˆ†ï¼‰

# ä½¿ç”¨è€…ï¼ˆå¦‚æœ App æ²’å‚³ worker_idï¼Œå°±ç”¨é€™å€‹ï¼‰
DEFAULT_WORKER_ID = os.environ.get("DEFAULT_WORKER_ID", "user001")

# é¢¨éšªå±¤ç´š â†’ é¡è‰²èˆ‡æ¨™ç±¤ï¼ˆæ”¹æˆå‰ç«¯æ…£ç”¨è‰²ç¥¨ï¼‰
RISK_LABELS = ["ä½åº¦", "ä¸­åº¦", "é«˜åº¦"]
RISK_COLORS = ["#22C55E", "#F59E0B", "#EF4444"]  # ç¶ ã€æ©˜ã€ç´…

# æ„Ÿæ¸¬å™¨è¼¸å‡ºæœ€å¤§å€¼æç¤ºï¼ˆè‹¥ç„¡è¨­å®šï¼Œç¨å¾Œæœƒä¾è³‡æ–™è‡ªå‹•æ¨æ–·ï¼‰
try:
    _sensor_max_env = os.environ.get("MVC_SENSOR_MAX")
    MVC_SENSOR_MAX_HINT: Optional[float] = None
    if _sensor_max_env is not None:
        candidate = float(_sensor_max_env)
        if candidate > 0:
            MVC_SENSOR_MAX_HINT = candidate
except Exception:
    MVC_SENSOR_MAX_HINT = None

# å°ç£æ™‚å€
TZ_TAIWAN = timezone(timedelta(hours=8))

# å¸¸æ•¸ï¼šå‹åˆ¥åç¨±èˆ‡ç¯©é¸é–€æª»
HEARTBEAT_TYPE = "heartbeat"
MIN_EFFECTIVE_PCT = 0.1  # ä½æ–¼æ­¤å€¼è¦–ç‚ºè¿‘ä¼¼ 0ï¼Œä¸ç´å…¥ç‰¹å¾µ
# è‹¥æ”¶åˆ°ç•°å¸¸å°çš„ UNIX ç§’ï¼ˆä¾‹å¦‚ 56.545 ç§’è¢«èª¤ç•¶ epochï¼‰ï¼Œä»¥ 2000-01-01 ä½œç‚ºã€Œåˆç†ç•Œç·šã€
UNREASONABLE_EPOCH_SEC = datetime(2000, 1, 1, tzinfo=timezone.utc).timestamp()


def now_taiwan_iso() -> str:
    return datetime.now(TZ_TAIWAN).isoformat()


# ==================== FastAPI ====================
app = FastAPI(title=APP_TITLE, version="v1.3")

# CORS è¨­å®šï¼šé è¨­å…¨é–‹ï¼ˆæ”¯æ´ä»¥é€—è™Ÿåˆ†éš”çš„å…è¨±ä¾†æºï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in os.environ.get("CORS_ALLOW_ORIGINS", "*").split(",")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== DB ====================
logger = logging.getLogger("uvicorn.error")


def get_conn() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH, timeout=5.0, check_same_thread=False)
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        conn.execute("PRAGMA busy_timeout=5000;")
    except Exception:
        pass
    return conn


def init_db() -> None:
    conn = get_conn()
    c = conn.cursor()
    # æ“´å……æ¬„ä½ä½†ç›¸å®¹ï¼šå¿…å¡« worker_id / timestamp / percent_mvc
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            worker_id TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            percent_mvc REAL NOT NULL,
            ts REAL,               -- UNIX secondsï¼Œå¯ç©º
            rms REAL,              -- é ç•™ EMG RMSï¼Œéå¿…å¡«
            type TEXT              -- ä¾†æº/é¡å‹ï¼ˆä¾‹å¦‚ 'heartbeat' / 'mvc' / 'emg' / 'imu'ï¼‰
        )
        """
    )
    # ä»¥é˜²èˆŠè¡¨ç¼ºæ¬„ä½ï¼ˆå¿½ç•¥éŒ¯èª¤å³å¯ï¼‰
    for ddl in (
        "ALTER TABLE sensor_data ADD COLUMN rms REAL",
        "ALTER TABLE sensor_data ADD COLUMN ts REAL",
        "ALTER TABLE sensor_data ADD COLUMN type TEXT",
    ):
        try:
            c.execute(ddl)
        except Exception:
            pass

    c.execute("CREATE INDEX IF NOT EXISTS idx_worker_ts ON sensor_data(worker_id, timestamp)")
    c.execute("CREATE INDEX IF NOT EXISTS idx_type_ts ON sensor_data(type, timestamp)")
    conn.commit()
    conn.close()


init_db()


@app.middleware("http")
async def log_requests(request: Request, call_next):
    body = await request.body()
    logger.info(
        f"[REQ] {request.method} {request.url.path} q={dict(request.query_params)} "
        f"len={len(body)} ct={request.headers.get('content-type')}"
    )
    try:
        response = await call_next(request)
        return response
    finally:
        try:
            status_code = getattr(response, "status_code", "n/a")
        except UnboundLocalError:
            status_code = "n/a"
        logger.info(f"[RESP] {request.method} {request.url.path} -> {status_code}")


@app.exception_handler(Exception)
async def unhandled_ex_handler(request: Request, exc: Exception):
    import traceback

    tb = "".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
    logger.error(f"[UNHANDLED] {request.url.path} {exc}\n{tb}")
    if isinstance(exc, HTTPException):
        detail = exc.detail
        if not isinstance(detail, dict):
            detail = {"detail": detail}
        return JSONResponse(
            status_code=exc.status_code,
            content=detail,
            headers=getattr(exc, "headers", None),
        )
    return JSONResponse(status_code=500, content={"error": "internal_error", "detail": str(exc)})


# ==================== è«‹æ±‚æ¨¡å‹ ====================
class SensorData(BaseModel):
    percent_mvc: float = Field(ge=0, le=120)  # æ­£è¦åŒ–å¾Œå…è¨±äº›å¾®è¶… 100
    timestamp: Optional[str] = None  # ISO-8601


# ==================== å·¥å…·ï¼šæ­£è¦åŒ–ä¸Šå‚³åˆ— ====================
def _to_ts_sec(v: Any) -> float:
    if v is None:
        return datetime.utcnow().timestamp()
    if isinstance(v, (int, float)):
        return float(v)
    # å¯èƒ½æ˜¯ ISO å­—ä¸²
    try:
        dt = datetime.fromisoformat(str(v).replace("Z", "+00:00"))
        return dt.timestamp()
    except Exception:
        return datetime.utcnow().timestamp()


def _safe_iso_from_ts(ts_sec: Optional[float]) -> str:
    """
    å°‡ tsï¼ˆUNIX ç§’æˆ– Noneï¼‰è½‰ç‚º ISO-8601 UTCã€‚
    è‹¥ ts ç§’æ•¸æ˜é¡¯ä¸åˆç†ï¼ˆå¦‚ 56.545ï¼‰ï¼Œæ”¹ç”¨ç¾åœ¨æ™‚é–“é˜²å‘†ã€‚
    """
    if ts_sec is None:
        ts_sec = datetime.utcnow().timestamp()
    try:
        tsf = float(ts_sec)
    except Exception:
        tsf = datetime.utcnow().timestamp()

    if tsf < UNREASONABLE_EPOCH_SEC:  # æ—©æ–¼ 2000-01-01 è¦–ç‚ºä¸åˆç†
        tsf = datetime.utcnow().timestamp()

    return datetime.utcfromtimestamp(tsf).replace(tzinfo=timezone.utc).isoformat()


def _clean_numeric(v: Any) -> Optional[float]:
    try:
        if v is None:
            return None
        return float(v)
    except Exception:
        return None


def _infer_mvc_scale(values: List[Optional[float]]) -> float:
    """æ ¹æ“šè³‡æ–™æ±ºå®šåŸå§‹ MVC çš„æœ€å¤§åˆ»åº¦ï¼ˆæ¨æ–· 0..1 / 0..20 / 0..100 é€™ç¨®ï¼‰ã€‚"""
    if MVC_SENSOR_MAX_HINT:
        return MVC_SENSOR_MAX_HINT

    cleaned = [float(x) for x in values if x is not None]
    if not cleaned:
        return 100.0

    max_val = max(cleaned)
    if max_val <= 1.0:
        return 1.0
    if max_val <= 20.0:
        return 20.0
    if max_val <= 120.0:
        return 100.0
    return max(100.0, max_val)


def _mvc_to_percent(raw_value: Optional[float], scale: float) -> float:
    if raw_value is None:
        return 0.0
    try:
        value = float(raw_value)
    except Exception:
        return 0.0

    scale = float(scale) if scale and scale > 0 else 100.0
    if scale == 1.0:
        percent = value * 100.0
    else:
        percent = (value / scale) * 100.0
    if percent < 0:
        percent = 0.0
    return float(min(percent, 120.0))  # å…è¨±äº›å¾®è¶… 100ï¼Œé¿å…éåº¦æˆªæ–·


def _to_float(v: Any) -> Optional[float]:
    try:
        return float(v)
    except Exception:
        return None


def _ensure_json_array(rows: Union[Dict[str, Any], List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    if rows is None:
        return []
    if isinstance(rows, list):
        return [dict(r) for r in rows]
    if isinstance(rows, dict):
        return [dict(rows)]
    raise HTTPException(400, detail="rows å¿…é ˆæ˜¯ç‰©ä»¶æˆ–ç‰©ä»¶é™£åˆ—")


def _normalize_rows(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å®¹éŒ¯éµåä¸¦è½‰æˆ DB æ¬„ä½ï¼š
    - worker_idï¼šé è¨­ DEFAULT_WORKER_ID
    - percent_mvcï¼šå®¹å¿ MVC(0..1/0..100)ã€percent_mvcã€mvcã€emg_pctï¼ˆè‡ªå‹•æ¨æ–·åˆ»åº¦å¾Œæ›ç®—ç‚º 0..100ï¼‰
    - timestampï¼šISOï¼›è‹¥æ²’çµ¦ï¼Œç”¨ tsï¼ˆç§’ï¼‰æˆ–ç¾åœ¨ï¼›ä¸¦é˜²å‘†ä¿®æ­£ã€Œä¸åˆç†çš„ epochã€
    - tsï¼šUNIX ç§’
    - rmsï¼šå®¹å¿ RMS/emg_rms/rms/emg
    - typeï¼šåŸæ¨£ä¿ç•™ï¼ˆheartbeat / mvc / emg / imuï¼‰
    """
    out: List[Dict[str, Any]] = []
    raw_mvc_values: List[Optional[float]] = []

    # å…ˆæƒä¸€æ¬¡ï¼šè’é›†åŸå§‹ mvc å€¼ä»¥æ¨æ–·åˆ»åº¦
    for m in rows:
        mm = dict(m)
        raw_mvc = None
        for k in ("percent_mvc", "MVC", "mvc", "emg_pct"):
            if k in mm and raw_mvc is None:
                raw_mvc = _clean_numeric(mm.get(k))
        raw_mvc_values.append(raw_mvc)

    scale = _infer_mvc_scale(raw_mvc_values)

    # å†å»ºæœ€çµ‚åˆ—
    for m, raw_mvc in zip(rows, raw_mvc_values):
        mm = dict(m)
        worker_id = str((mm.get("worker_id") or DEFAULT_WORKER_ID)).strip()

        # æ™‚é–“
        if mm.get("timestamp"):
            ts_sec = _to_ts_sec(mm["timestamp"])
        else:
            ts_sec = _to_ts_sec(mm.get("ts"))
        iso = _safe_iso_from_ts(ts_sec)

        # RMSï¼ˆå¯é¸ï¼‰
        rms = None
        for k in ("RMS", "emg_rms", "rms", "emg"):
            if k in mm and rms is None:
                rms = _to_float(mm.get(k))

        typ = str(mm.get("type")) if mm.get("type") is not None else None

        out.append(
            {
                "worker_id": worker_id,
                "timestamp": iso,
                "percent_mvc": _mvc_to_percent(raw_mvc, scale) if raw_mvc is not None else 0.0,
                "ts": float(ts_sec) if ts_sec is not None else None,
                "rms": rms,
                "type": typ,
            }
        )

    return out


# ==================== æ¨¡å‹ï¼ˆè¼‰å…¥/è¨“ç·´ï¼‰ ====================
def _train_rf_classifier() -> RandomForestClassifier:
    # åˆæˆä¸€ä»½å¯å€åˆ† Î”%MVC èˆ‡è®ŠåŒ–é€Ÿç‡çš„è³‡æ–™ï¼Œè¨“ç·´å¿«é€Ÿ
    n = 3000
    rng = np.random.RandomState(42)
    current_mvc = rng.uniform(20, 95, size=n)
    total_change = rng.uniform(-10, 60, size=n)
    change_rate = rng.uniform(-1.0, 3.0, size=n)
    avg_mvc = rng.uniform(25, 85, size=n)
    std_mvc = rng.uniform(0.5, 15, size=n)

    X = np.vstack([current_mvc, total_change, change_rate, avg_mvc, std_mvc]).T
    y = np.zeros(n, dtype=int)
    y[(total_change >= 20)] = 1
    y[(total_change >= 40)] = 2
    y[(change_rate > 2.0) & (y < 2)] = 2
    y[(change_rate > 1.2) & (y < 1)] = 1

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(
        n_estimators=150, max_depth=14, min_samples_split=8, min_samples_leaf=4, random_state=42
    )
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    print(f"âœ… RandomForest è¨“ç·´å®Œæˆï¼ˆval acc = {score:.3f}ï¼‰")
    joblib.dump(model, RF_MODEL_PATH)
    return model


def _load_or_train_rf() -> RandomForestClassifier:
    if os.path.exists(RF_MODEL_PATH):
        try:
            print("âœ… è¼‰å…¥ RandomForest")
            return joblib.load(RF_MODEL_PATH)
        except Exception:
            pass
    print("âš™ï¸ æœªæ‰¾åˆ°æ¨¡å‹ï¼Œé–‹å§‹è¨“ç·´ RandomForest ...")
    return _train_rf_classifier()


RF_MODEL: RandomForestClassifier = _load_or_train_rf()

# ====== å˜—è©¦è¼‰å…¥ LSTMï¼ˆå¯é¸ï¼‰======
_TF_AVAILABLE = False
_LSTM_MODEL = None
_SCALER = None
_LSTM_SOURCE = "naive"

try:
    # å»¶é²åŒ¯å…¥ï¼Œé¿å…ç’°å¢ƒæ²’è£ TF å°±å´©æ½°
    import tensorflow as tf  # type: ignore
    from tensorflow.keras.models import load_model  # type: ignore

    if os.path.exists(LSTM_MODEL_PATH) and os.path.exists(SCALER_PATH):
        _LSTM_MODEL = load_model(LSTM_MODEL_PATH)
        _SCALER = joblib.load(SCALER_PATH)
        _TF_AVAILABLE = True
        _LSTM_SOURCE = "lstm"
        print("âœ… LSTM èˆ‡ SCALER è¼‰å…¥æˆåŠŸ")
    else:
        print("â„¹ï¸ æ‰¾ä¸åˆ° LSTM/SCALERï¼Œå°‡ä½¿ç”¨å¾Œæ´é æ¸¬ï¼ˆnaive linearï¼‰")
except Exception as _e:
    print(f"â„¹ï¸ TensorFlow ç„¡æ³•è¼‰å…¥æˆ–æ¨¡å‹ç¼ºå¤±ï¼ˆ{_e}ï¼‰ï¼Œä½¿ç”¨å¾Œæ´é æ¸¬ï¼ˆnaive linearï¼‰")


# ==================== ç‰¹å¾µè¨ˆç®— & é¢¨éšªè©•åˆ† ====================
def _get_df(worker_id: str, limit: int = 600, exclude_heartbeat: bool = True) -> pd.DataFrame:
    """
    æ‹‰è³‡æ–™ï¼ˆé è¨­æ’é™¤ heartbeatï¼‰ï¼Œå–æœ€æ–° limit ç­†ï¼Œå†è½‰å›æ™‚é–“æ­£åºã€‚
    """
    conn = get_conn()
    if exclude_heartbeat:
        sql = """
        SELECT * FROM sensor_data
        WHERE worker_id = ? AND (type IS NULL OR type != ?)
        ORDER BY timestamp DESC LIMIT ?
        """
        params = (worker_id, HEARTBEAT_TYPE, int(limit))
    else:
        sql = """
        SELECT * FROM sensor_data
        WHERE worker_id = ?
        ORDER BY timestamp DESC LIMIT ?
        """
        params = (worker_id, int(limit))

    df = pd.read_sql_query(sql, conn, params=params)
    conn.close()

    if not df.empty:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
        df = df.dropna(subset=["timestamp"])
        df = df.sort_values("timestamp").reset_index(drop=True)
    return df


def _extract_features(df: pd.DataFrame) -> Optional[np.ndarray]:
    if len(df) < 2:
        return None

    # æ•´ç† & ä¿éšª
    df = df.dropna(subset=["timestamp", "percent_mvc"]).copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, errors="coerce")
    df = df.sort_values("timestamp").reset_index(drop=True)
    df["ts_sec"] = df["timestamp"].astype("int64") / 1e9  # UNIX ç§’

    # --- 1) ç”¨è¼ƒé•·çš„æ™‚é–“çª—ä¼°è®ŠåŒ–é€Ÿç‡ï¼ˆé è¨­ 5 åˆ†é˜ï¼›ä¸è¶³å‰‡é€€å›æœ€å¤š 30 ç­†ï¼‰---
    t_end = df["timestamp"].iloc[-1]
    window = df[df["timestamp"] >= t_end - pd.Timedelta(minutes=5)]
    if len(window) < 5:
        window = df.tail(min(30, len(df)))

    # --- 2) å…¶ä»–ç‰¹å¾µ ---
    first_mvc = float(df.iloc[0]["percent_mvc"])
    last_mvc = float(df.iloc[-1]["percent_mvc"])
    initial_mvc = max(first_mvc, float(df["percent_mvc"].max()))
    current_mvc = last_mvc
    total_change = float(initial_mvc - current_mvc)

    # --- 3) ç”¨æœ€å°äºŒä¹˜å›æ­¸ç®—æ–œç‡ï¼ˆ%/minï¼‰ï¼Œä¸¦åšç©©å®šåŒ– ---
    if len(window) >= 2:
        x = (window["ts_sec"] - window["ts_sec"].iloc[0]).to_numpy()  # ç§’å·®ï¼ˆå¾ 0 é–‹å§‹ï¼‰
        y = window["percent_mvc"].to_numpy()

        span_min = (x[-1] - x[0]) / 60.0  # çª—å£æ¶µè“‹çš„åˆ†é˜æ•¸
        if span_min <= 0:
            change_rate = 0.0
        else:
            # ä¸­å¿ƒåŒ–å¾Œçš„ä¸€å…ƒç·šæ€§å›æ­¸æ–œç‡ï¼ˆ%/ç§’ï¼‰
            x_c = x - x.mean()
            y_c = y - y.mean()
            denom = float((x_c * x_c).sum())
            slope_per_sec = float((x_c * y_c).sum() / denom) if denom > 0 else 0.0
            change_rate = slope_per_sec * 60.0  # æ›æˆ %/min
            change_rate = float(np.clip(change_rate, -2.0, 2.0))
    else:
        change_rate = 0.0

    # --- 4) å‡å€¼/æ³¢å‹• ---
    avg_mvc = float(window["percent_mvc"].mean())
    std_mvc = float(window["percent_mvc"].std(ddof=0)) if len(window) > 1 else 0.0

    # å’Œè¨“ç·´æ™‚çš„ç‰¹å¾µé †åºä¸€è‡´
    X = np.array([[current_mvc, total_change, change_rate, avg_mvc, std_mvc]])
    return X


def _predict_risk(features: np.ndarray) -> Tuple[str, int, str, np.ndarray]:
    """
    å›å‚³ï¼š(label, level012, color, proba)
    level012: 0=ä½ 1=ä¸­ 2=é«˜
    """
    level = int(RF_MODEL.predict(features)[0])
    proba = RF_MODEL.predict_proba(features)[0]
    return RISK_LABELS[level], level, RISK_COLORS[level], proba


def _level13_from_012(level012: int) -> int:
    # å°å¤– 1/2/3
    return int(level012) + 1


# ==================== LSTM / å¾Œæ´ é æ¸¬ ====================
def _resample_minutely(df: pd.DataFrame, step_min: int) -> pd.DataFrame:
    """æŠŠ percent_mvc ä¾åˆ†é˜é‡æ¡æ¨£ï¼ˆå‰å‘å¡«è£œï¼‰ï¼Œé¿å… LSTM/å¤–æ¨è¢«ä¸ç­‰é–“éš”å¹²æ“¾ã€‚"""
    s = df.set_index("timestamp")["percent_mvc"].astype(float)
    s = s.resample(f"{step_min}T").ffill().dropna()
    out = s.to_frame(name="percent_mvc").reset_index()
    return out


def _prepare_seq_for_lstm(df: pd.DataFrame) -> Optional[np.ndarray]:
    """
    å–æœ€è¿‘ HISTORY_SEQUENCE_LENGTH ç­†ï¼ˆåˆ†é˜å–æ¨£ï¼‰ï¼Œå›å‚³ shape=(1, L, 1)
    """
    if df.empty:
        return None
    rs = _resample_minutely(df, TIME_INTERVAL_MINUTES)
    if len(rs) < HISTORY_SEQUENCE_LENGTH:
        return None
    window = rs.tail(HISTORY_SEQUENCE_LENGTH)["percent_mvc"].to_numpy().astype("float32").reshape(-1, 1)
    if _TF_AVAILABLE and _SCALER is not None:
        x_scaled = _SCALER.transform(window)  # (L, 1)
    else:
        mean = window.mean()
        std = window.std() if window.std() > 1e-6 else 1.0
        x_scaled = (window - mean) / std
    return x_scaled.reshape(1, HISTORY_SEQUENCE_LENGTH, 1)


def _forecast_with_lstm(df: pd.DataFrame) -> Tuple[List[float], str]:
    """
    å›å‚³ï¼ˆæœªä¾† PREDICTION_HORIZON å€‹ %MVCã€ä¾†æºï¼‰
    """
    X = _prepare_seq_for_lstm(df)
    if X is None:
        raise ValueError("è³‡æ–™ä¸è¶³ä»¥åšåºåˆ—é æ¸¬")
    if _TF_AVAILABLE and _LSTM_MODEL is not None:
        y_hat = _LSTM_MODEL.predict(X, verbose=0).reshape(-1).astype(float)
        forecast = [float(np.clip(v, 0.0, 120.0)) for v in y_hat[:PREDICTION_HORIZON]]
        return forecast, "lstm"
    # ---- å¾Œæ´ï¼šç·šæ€§è¶¨å‹¢å¤–æ¨ ----
    rs = _resample_minutely(df, TIME_INTERVAL_MINUTES)
    y = rs.tail(HISTORY_SEQUENCE_LENGTH)["percent_mvc"].to_numpy().astype(float)
    x = np.arange(len(y), dtype=float)
    x_c = x - x.mean()
    denom = float((x_c * x_c).sum()) or 1.0
    slope = float(((x_c * (y - y.mean())).sum()) / denom)
    intercept = float(y.mean() - slope * x.mean())
    future_x = np.arange(len(y), len(y) + PREDICTION_HORIZON, dtype=float)
    y_future = intercept + slope * future_x
    y_future = np.clip(y_future, 0.0, 120.0)
    return [float(v) for v in y_future], "naive"


def _risk_for_future_points(df_hist: pd.DataFrame, future_vals: List[float]) -> Dict[int, Dict[str, Any]]:
    """
    æŠŠé æ¸¬çš„æœªä¾†é»æ¥åˆ°æ­·å²åºåˆ—å¾Œï¼ŒæŒ‘é¸ 5/15/30 åˆ†é˜ä½ç½®åš RF é¢¨éšªåˆ†ç´šã€‚
    å›å‚³ï¼š{5: {...}, 15: {...}, 30: {...}}
    """
    rs = _resample_minutely(df_hist, TIME_INTERVAL_MINUTES)
    base_ts = rs["timestamp"].iloc[-1]
    results: Dict[int, Dict[str, Any]] = {}
    for m in [5, 15, 30]:
        if m <= 0 or m > len(future_vals):
            continue
        fut = pd.DataFrame(
            {
                "timestamp": [base_ts + pd.Timedelta(minutes=i + 1) for i in range(m)],
                "percent_mvc": future_vals[:m],
            }
        )
        df_tmp = pd.concat([rs[["timestamp", "percent_mvc"]], fut], ignore_index=True)
        feats = _extract_features(df_tmp)
        if feats is None:
            continue
        label, level012, color, proba = _predict_risk(feats)
        results[m] = {
            "label": label,
            "level": _level13_from_012(level012),
            "color": color,
            "prob": [float(x) for x in proba],
        }
    return results


# ==================== è·¯ç”± ====================
@app.get("/")
def home():
    return {
        "service": APP_TITLE,
        "version": "v1.3",
        "build": APP_BUILD,
        "endpoints": {
            "å¥åº·æª¢æŸ¥": "GET /healthz",
            "ç¸½è¦½": "GET /health",
            "App å°ˆç”¨": "GET /app_data",
            "æœªä¾†é æ¸¬": "GET /forecast",
            "å–®ç­†ä¸Šå‚³": "POST /upload",
            "æ‰¹æ¬¡ä¸Šå‚³": "POST /process_json",
            "æŸ¥è©¢ç‹€æ…‹": "GET /status/{worker_id}",
            "æ¸…ç©ºè³‡æ–™": "DELETE /clear/{worker_id}",
        },
    }


@app.get("/healthz")
def healthz():
    return {"status": "ok", "build": APP_BUILD}


@app.get("/health")
def health():
    conn = get_conn()
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM sensor_data")
    total = c.fetchone()[0]
    conn.close()
    return {
        "status": "healthy",
        "db": {"path": DB_PATH, "total_records": total},
        "models": {
            "rf_loaded": RF_MODEL is not None,
            "rf_path": RF_MODEL_PATH,
            "lstm_loaded": bool(_TF_AVAILABLE and _LSTM_MODEL is not None and _SCALER is not None),
            "lstm_path": LSTM_MODEL_PATH if os.path.exists(LSTM_MODEL_PATH) else None,
        },
        "forecast": {
            "history_len": HISTORY_SEQUENCE_LENGTH,
            "horizon_min": PREDICTION_HORIZON,
            "step_min": TIME_INTERVAL_MINUTES,
            "source": _LSTM_SOURCE,
        },
        "version": "v1.3",
        "build": APP_BUILD,
    }


# ---- å–®ç­†ä¸Šå‚³ï¼ˆä½ åŸæœ¬çš„ /uploadï¼‰ ----
@app.post("/upload")
def upload(item: SensorData):
    worker_id = DEFAULT_WORKER_ID
    ts_iso = item.timestamp or now_taiwan_iso()

    conn = get_conn()
    c = conn.cursor()
    c.execute(
        "INSERT INTO sensor_data(worker_id, timestamp, percent_mvc, ts, rms, type) VALUES(?,?,?,?,?,?)",
        (worker_id, ts_iso, float(item.percent_mvc), None, None, "mvc"),
    )
    conn.commit()
    conn.close()

    return {"status": "success", "worker_id": worker_id, "timestamp": ts_iso, "mvc": float(item.percent_mvc)}


# ---- App ç°¡åŒ–ç«¯é»ï¼ˆå¤šå› forecastï¼‰----
@app.get("/app_data")
def get_app_data():
    df = _get_df(DEFAULT_WORKER_ID, limit=120, exclude_heartbeat=True)
    if df.empty:
        return {
            "user_id": DEFAULT_WORKER_ID,
            "last_update": None,
            "risk_level": "ç„¡è³‡æ–™",
            "forecast": None,
        }

    feats = _extract_features(df)
    if feats is None:
        forecast_vals = None
        source = _LSTM_SOURCE
        try:
            vals, source = _forecast_with_lstm(df)
            forecast_vals = vals
        except Exception:
            forecast_vals = None
        return {
            "user_id": DEFAULT_WORKER_ID,
            "last_update": str(df.iloc[-1]["timestamp"]),
            "risk_level": "è³‡æ–™ä¸è¶³",
            "forecast": None
            if forecast_vals is None
            else {
                "horizon_min": min(PREDICTION_HORIZON, len(forecast_vals)),
                "step_min": TIME_INTERVAL_MINUTES,
                "values": forecast_vals,
                "source": source,
            },
        }

    label, level, color, _ = _predict_risk(feats)

    forecast_block = None
    try:
        vals, source = _forecast_with_lstm(df)
        risks = _risk_for_future_points(df, vals)
        forecast_block = {
            "horizon_min": min(PREDICTION_HORIZON, len(vals)),
            "step_min": TIME_INTERVAL_MINUTES,
            "values": vals,
            "source": source,
            "risk_at_min": risks,  # {5:{...},15:{...},30:{...}}
        }
    except Exception:
        forecast_block = None

    return {
        "user_id": DEFAULT_WORKER_ID,
        "last_update": str(df.iloc[-1]["timestamp"]),
        "risk_level": label,
        "forecast": forecast_block,
    }


# ---- ç¨ç«‹çš„æœªä¾†é æ¸¬ç«¯é» ----
@app.get("/forecast")
def forecast(worker_id: Optional[str] = None):
    wid = str((worker_id or DEFAULT_WORKER_ID)).strip()
    df = _get_df(wid, limit=HISTORY_SEQUENCE_LENGTH * 4, exclude_heartbeat=True)
    if df.empty or len(df) < max(5, HISTORY_SEQUENCE_LENGTH // 4):
        raise HTTPException(422, detail="è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæœªä¾†é æ¸¬")

    vals, source = _forecast_with_lstm(df)
    risks = _risk_for_future_points(df, vals)
    last_ts = str(df.iloc[-1]["timestamp"])
    return {
        "worker_id": wid,
        "last_update": last_ts,
        "horizon_min": min(PREDICTION_HORIZON, len(vals)),
        "step_min": TIME_INTERVAL_MINUTES,
        "values": vals,
        "source": source,  # "lstm" æˆ– "naive"
        "risk_at_min": risks,  # {5:{label,level,color,prob}, 15:{...}, 30:{...}}
    }


# ---- æ‰¹æ¬¡ä¸Šå‚³ï¼ˆFlutter App ç”¨ï¼‰----
@app.post("/process_json")
def process_json(rows: Union[List[Dict[str, Any]], Dict[str, Any]]):
    payload = _ensure_json_array(rows)

    # æ­£è¦åŒ– + åˆ»åº¦æ›ç®— + æ™‚é–“ä¿®æ­£ï¼ˆé˜² 1970ï¼‰
    normalized = _normalize_rows(payload)
    if not normalized:
        raise HTTPException(400, detail="ç©ºçš„ä¸Šå‚³è³‡æ–™")

    conn = get_conn()
    c = conn.cursor()
    inserted = 0
    last_worker = None

    try:
        for r in normalized:
            worker_id = str((r.get("worker_id") or DEFAULT_WORKER_ID)).strip()
            ts_iso = r.get("timestamp") or now_taiwan_iso()
            pmv = r.get("percent_mvc")
            pmv = float(pmv) if pmv is not None else 0.0
            ts_sec = r.get("ts")
            ts_sec = float(ts_sec) if ts_sec is not None else None
            rms = r.get("rms")
            rms = float(rms) if rms is not None else None
            typ = r.get("type")

            c.execute(
                "INSERT INTO sensor_data(worker_id, timestamp, percent_mvc, ts, rms, type) VALUES(?,?,?,?,?,?)",
                (worker_id, ts_iso, pmv, ts_sec, rms, typ),
            )
            inserted += 1
            last_worker = worker_id
        conn.commit()
    except sqlite3.OperationalError as e:
        raise HTTPException(status_code=503, detail=f"db_error: {e}")
    finally:
        try:
            conn.close()
        except Exception:
            pass

    # å›å‚³åŒæ™‚é™„ä¸Šæœ€æ–°é¢¨éšªç­‰ç´šï¼ˆæ–¹ä¾¿ App ç«‹å³æ›´æ–°ï¼‰
    if last_worker is None:
        last_worker = DEFAULT_WORKER_ID
    df = _get_df(last_worker, limit=120, exclude_heartbeat=True)
    if df.empty:
        resp = {"status": "ok", "inserted": inserted, "worker_id": last_worker}
    else:
        feats = _extract_features(df)
        if feats is None:
            resp = {
                "status": "ok",
                "inserted": inserted,
                "worker_id": last_worker,
                "risk": {"label": "è³‡æ–™ä¸è¶³", "level": 1, "color": "#95a5a6"},
            }
        else:
            label012, level012, color, proba = _predict_risk(feats)
            resp = {
                "status": "ok",
                "inserted": inserted,
                "worker_id": last_worker,
                "risk": {
                    "label": label012,
                    "level": _level13_from_012(level012),  # 1/2/3
                    "color": color,
                    "prob": [float(x) for x in proba],
                },
            }
    return resp


# ---- æŸ¥è©¢ç‹€æ…‹ï¼ˆApp æ‹‰å¿ƒè·³/ç‹€æ…‹ç”¨ï¼‰----
@app.get("/status/{worker_id}")
def status(worker_id: str):
    wid = str((worker_id or DEFAULT_WORKER_ID)).strip()
    df = _get_df(wid, limit=120, exclude_heartbeat=True)
    if df.empty:
        return {
            "worker_id": wid,
            "status": "ç„¡è³‡æ–™",
            "risk_label": "ä½åº¦",
            "risk_level": 1,
            "risk_color": RISK_COLORS[0],
            "last": None,
        }

    feats = _extract_features(df)
    if feats is None:
        last = df.iloc[-1]
        return {
            "worker_id": wid,
            "status": "è³‡æ–™ä¸è¶³",
            "risk_label": "ä½åº¦",
            "risk_level": 1,
            "risk_color": RISK_COLORS[0],
            "last": {
                "timestamp": str(last["timestamp"]),
                "percent_mvc": float(last["percent_mvc"]),
                "rms": float(last["rms"]) if pd.notna(last.get("rms", np.nan)) else None,
            },
        }

    label012, level012, color, proba = _predict_risk(feats)
    last = df.iloc[-1]
    return {
        "worker_id": wid,
        "status": "ok",
        "risk_label": label012,
        "risk_level": _level13_from_012(level012),  # 1/2/3
        "risk_color": color,
        "prob": [float(x) for x in proba],
        "last": {
            "timestamp": str(last["timestamp"]),
            "percent_mvc": float(last["percent_mvc"]),
            "rms": float(last["rms"]) if pd.notna(last.get("rms", np.nan)) else None,
        },
    }


# ---- æ¸…ç©ºè³‡æ–™ï¼ˆæ¸¬è©¦ç”¨ï¼‰----
@app.delete("/clear/{worker_id}")
def clear(worker_id: str):
    wid = str((worker_id or DEFAULT_WORKER_ID)).strip()
    conn = get_conn()
    c = conn.cursor()
    c.execute("DELETE FROM sensor_data WHERE worker_id = ?", (wid,))
    deleted = c.rowcount
    conn.commit()
    conn.close()
    return {"status": "success", "deleted": deleted, "worker_id": wid}


# ==================== æœ¬æ©Ÿå•Ÿå‹•æç¤º ====================
if __name__ == "__main__":
    print("ğŸš€ ç–²å‹é æ¸¬ç³»çµ±å•Ÿå‹•")
    print(f"ğŸ“¦ DB: {DB_PATH}")
    print(f"ğŸ‘¤ é è¨­ä½¿ç”¨è€…: {DEFAULT_WORKER_ID}")
    print(f"ğŸ§± Build: {APP_BUILD}")
    print("ğŸ“ æœ¬æ©Ÿ: http://localhost:8000")
    print("â¡ï¸  å¥åº·æª¢æŸ¥:   GET http://localhost:8000/healthz")
    print("â¡ï¸  æ‰¹æ¬¡ä¸Šå‚³:   POST http://localhost:8000/process_json")
    print("â¡ï¸  æŸ¥è©¢ç‹€æ…‹:   GET  http://localhost:8000/status/user001")
    print("â¡ï¸  App ç°¡åŒ–:   GET  http://localhost:8000/app_data")
    print("â¡ï¸  æœªä¾†é æ¸¬:   GET  http://localhost:8000/forecast")
    print("\nå•Ÿå‹•ï¼šuvicorn main:app --reload --host 0.0.0.0 --port 8000")
